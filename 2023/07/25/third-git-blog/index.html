
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>第二章 改善深层神经网络：超参数调试、正则化及优化 | 熊妞随手记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="改善深层神经网络：超参数调试、正则化及优化概述  通过第一门课的学习，我们对于神经网络的基本概念，逻辑回归和神经网络的类比，以及浅层和深层神经网络有了了解。这门课针对深层神经网络的超参数调试、正则化以及优化做进一步介绍，以便在实际应用的时候，对于超参数的选择，提供了很多方法和例子，加深我们的理解。有效的运作神经网络主要包括：超参数调优、如何构建数据、如何确保优化算法正常运行，从而使学习算法在合理时">
<meta property="og:type" content="article">
<meta property="og:title" content="第二章 改善深层神经网络：超参数调试、正则化及优化">
<meta property="og:url" content="http://example.com/2023/07/25/third-git-blog/index.html">
<meta property="og:site_name" content="熊妞随手记">
<meta property="og:description" content="改善深层神经网络：超参数调试、正则化及优化概述  通过第一门课的学习，我们对于神经网络的基本概念，逻辑回归和神经网络的类比，以及浅层和深层神经网络有了了解。这门课针对深层神经网络的超参数调试、正则化以及优化做进一步介绍，以便在实际应用的时候，对于超参数的选择，提供了很多方法和例子，加深我们的理解。有效的运作神经网络主要包括：超参数调优、如何构建数据、如何确保优化算法正常运行，从而使学习算法在合理时">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-25T08:00:00.000Z">
<meta property="article:modified_time" content="2023-07-26T10:40:14.640Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="深度挖掘">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="熊妞随手记" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">熊妞随手记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="example.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main"><article id="post-third-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/25/third-git-blog/" class="article-date">
  <time datetime="2023-07-25T08:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82/">参数调节</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第二章 改善深层神经网络：超参数调试、正则化及优化
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="改善深层神经网络：超参数调试、正则化及优化"><a href="#改善深层神经网络：超参数调试、正则化及优化" class="headerlink" title="改善深层神经网络：超参数调试、正则化及优化"></a>改善深层神经网络：超参数调试、正则化及优化</h1><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>  通过第一门课的学习，我们对于神经网络的基本概念，逻辑回归和神经网络的类比，以及浅层和深层神经网络有了了解。这门课针对深层神经网络的超参数调试、正则化以及优化做进一步介绍，以便在实际应用的时候，对于超参数的选择，提供了很多方法和例子，加深我们的理解。有效的运作神经网络主要包括：超参数调优、如何构建数据、如何确保优化算法正常运行，从而使学习算法在合理时间内完成自我学习。  </p>
<h1 id="详情"><a href="#详情" class="headerlink" title="详情"></a>详情</h1><hr>
<h2 id="构建数据"><a href="#构建数据" class="headerlink" title="构建数据"></a>构建数据</h2><table>
    <tr>
        <td></td> <td>功能</td> <td>数据来源</td>  <td>小数据量</td> <td>大数据量</td>
    </tr>
    <tr>
        <td>训练集</td> <td>训练模型</td> <td></td>  <td>70%</td> <td>98%</td>
    </tr>
    <tr>
        <td>验证集</td> <td>选择模型</td> <td>保证与测试集同分布</td>  <td>&lt;10%~20%</td> <td>1%</td>
    </tr>
    <tr>
        <td>测试集</td> <td>评估模型</td> <td>保证与验证集同分布</td>  <td>&lt;10%~20%</td> <td>1%</td>
    </tr>
</table>

<h2 id="偏差误差"><a href="#偏差误差" class="headerlink" title="偏差误差"></a>偏差误差</h2><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><pre><code>        * 初始模型训练完成后，评估算法的偏差高不高  
</code></pre><p>偏差 高：评估训练集或训练数据的性能；</p>
<hr>
<p>如果无法拟合训练集，则需要<strong>选择一个新网络</strong>或者<strong>花更多时间训练网络</strong>或者<strong>尝试更先进的优化算法</strong>。<br>本阶段需达成的目标是：<strong>拟合训练集</strong>。</p>
<pre><code>        * **偏差**降低到可接受程度后进行**方差**评估  
</code></pre><p><strong>方差</strong>高：最好的办法是<strong>采用更多的数据</strong>；当无法获得更多数据的时候，采用<strong>正则化</strong>来减少过拟合。</p>
<pre><code>        * 最终目标  ：**低偏差、低方差** 框架。
</code></pre><h3 id="偏差分析"><a href="#偏差分析" class="headerlink" title="偏差分析"></a>偏差分析</h3><pre><code>      collapsed:: true
      神经网络训练的流程显示模型训练的目标是**低偏差、低方差**，需依据偏差、方差的高低调整训练策略。  

        * **偏差：** 训练集误差与 **最优误差（贝叶斯误差）** 的比较结果；  
</code></pre><p>最优误差：通常采用人的识别误差，如误差为0。</p>
<pre><code>        * **方差：**验证集误差与 **最优误差（贝叶斯误差）** 的比较结果；
</code></pre><table>
<tr>
<td>误差分析</td> <td>特征</td> <td>处理</td>
</tr>
<tr>
<td>欠拟合</td> <td>偏差大：训练集误差大于最优误差  </td> <td><br>选择新网络<br><br>延长训练时间<br><br>尝试新的优化算法<br></td>
</tr>
<tr>
<td>过拟合</td> <td>方差大：验证集误差大于最优误差；<br>偏差小<br></td> <td><br>采用更多的数据训练<br><br>正则化<br></td>
</tr>
</table>

<h4 id="欠拟合问题处理"><a href="#欠拟合问题处理" class="headerlink" title="欠拟合问题处理"></a>欠拟合问题处理</h4><pre><code>          消除欠拟合是模型训练的第一步目标，至少要拟合训练集，如无法达到则说明“方向”性错误。  
</code></pre><h4 id="过拟合问题处理"><a href="#过拟合问题处理" class="headerlink" title="过拟合问题处理"></a>过拟合问题处理</h4><pre><code>            * 更多的训练数据
            * 正则化（当无法获取更多数据时）
            * 数据生成
</code></pre><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="（一）L2正则化"><a href="#（一）L2正则化" class="headerlink" title="（一）L2正则化"></a>（一）L2正则化</h3><h4 id="1、逻辑回归的正则化"><a href="#1、逻辑回归的正则化" class="headerlink" title="1、逻辑回归的正则化"></a>1、逻辑回归的正则化</h4><script type="math/tex; mode=display">
              \text L2正则化：
              J(W, b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\mid\mid W \mid\mid_2^{2} \\
              \text 向量参数 W 的 L2范数： \mid\mid W \mid\mid_2^{2} = W^{T}W = \sum_{j=1}^{n_{x}}w_j^{2} \\
              W\in \mathbb{R}^{n_x}, b\in\mathbb{R} \\
              \\
              \text L1正则化：
              J(W, b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\mid\mid W \mid\mid_1 \\
              \text 向量参数 W 的 L1范数： \mid\mid W \mid\mid_1 = \sum_{j=1}^{n_{x}}\mid w_j\mid \\</script><h4 id="2、神经网络的正则化"><a href="#2、神经网络的正则化" class="headerlink" title="2、神经网络的正则化"></a>2、神经网络的正则化</h4><script type="math/tex; mode=display">
              \text L2正则化：
              J(W^{[1]}, b^{[1]},...,W^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\sum_{l=1}^L\mid\mid W^{[l]} \mid\mid_F^{2} \\
              \text 矩阵范数（费罗贝尼乌斯范数）被定义为矩阵中所有元素的平方和： \mid\mid W^{[l]} \mid\mid_F^{2} = \sum_{i=1}^{n^{l-1}}\sum_{j=1}^{n^{l}}(w_{ij}^{[l]})^{2}</script><h4 id="包含正则化项的梯度下降"><a href="#包含正则化项的梯度下降" class="headerlink" title="包含正则化项的梯度下降"></a>包含正则化项的梯度下降</h4><pre><code>          collapsed:: true
</code></pre><script type="math/tex; mode=display">
              dW^{[l]} = (From Back Prop) + \frac{\lambda}{m}W^{[l]} \\
              \text参数更新：W^{[l]} = W^{[l]} - \alpha \times [d(From Back Prop) + \frac{\lambda}{m}W^{[l]}] \\
              = W^{[l]} - \alpha\frac{\lambda}{m}W^{[l]} - \alpha \times d(From Back Prop) \\
              = (1-\alpha\frac{\lambda}{m})W^{[l]} - \alpha \times d(From Back Prop)  \tag1</script><pre><code>          从公式（1）可以看到，使用L2正则化后， $W^&#123;[l]&#125;$ 更新时先乘了一个小于 1 的系数 $1-\alpha\frac&#123;\lambda&#125;&#123;m&#125;$ ，由此L2正则化也被称作**权重衰减**。  
</code></pre><hr>
<h5 id="正则化预防过拟合原理"><a href="#正则化预防过拟合原理" class="headerlink" title="正则化预防过拟合原理"></a>正则化预防过拟合原理</h5><script type="math/tex; mode=display">
</script><pre><code>                1）由公式（1）可直观理解，当 $\lambda$ 足够大，权重矩阵W被设置为接近于0的值，即把多隐藏单元的权重设置为0，极限情况下神经网络会变成一个小的网络，如同一个逻辑回归单元，可是深度却很大，这会使网络从过拟合状态更接近高偏差状态。  
                  2）权重参数W越小，则Z值也越小，其所在的取值区间将偏向于激活函数（如： $\tanh$ ）的线性变换部分，这也能防止过拟合的发生。 
</code></pre><hr>
<h3 id="（二）Dropout（随机失活）"><a href="#（二）Dropout（随机失活）" class="headerlink" title="（二）Dropout（随机失活）"></a>（二）Dropout（随机失活）</h3><pre><code>      collapsed:: true
</code></pre><h4 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h4><p>遍历网络的每一层，设置消除神经网络节点的概率；消除一些节点，得到一个节点更少，规模更小的网络；用backprop方法进行训练。</p>
<h4 id="dropout实施"><a href="#dropout实施" class="headerlink" title="dropout实施"></a>dropout实施</h4><p>反向随机失活(Inverted dropout)</p>
<pre><code>            1. 定义向量 $d3$ :
              <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>], a3.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
            1. d3 与 keep-prob 值比较，得到**新的 d3**，keep-prob在网络的不同层可以是不同的
            2. $a^&#123; [3]&#125;$ 计算激活值
              <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 = np.multiply(a3, d3)</span><br></pre></td></tr></table></figure>
            1. 扩展 $a^&#123; [3]&#125; $以保证$ a^&#123; [3]&#125; $期望值不变
              <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 /= keep-prob</span><br></pre></td></tr></table></figure>
            1. 在**测试阶段** 不使用dropout，因为不期望输出结果是随机的。当keep-prob值为1时，保留所有神经元。
</code></pre><h4 id="dropout-预防过拟合原理"><a href="#dropout-预防过拟合原理" class="headerlink" title="dropout 预防过拟合原理"></a>dropout 预防过拟合原理</h4><pre><code>            dropout 正则化相当于在每一次迭代过程都是训练一个以当前网络神经元子集所组成的一个新的简化的网络（越简单的网络越容易是欠拟合）。
</code></pre><h4 id="dropout-所带来的问题"><a href="#dropout-所带来的问题" class="headerlink" title="dropout 所带来的问题"></a>dropout 所带来的问题</h4><pre><code>            代价函数将不被明确定义，因为在每次迭代都会随机移除一些节点。
</code></pre><h3 id="（三）其他正则化方法"><a href="#（三）其他正则化方法" class="headerlink" title="（三）其他正则化方法"></a>（三）其他正则化方法</h3><pre><code>      collapsed:: true
</code></pre><h4 id="1、数据扩增"><a href="#1、数据扩增" class="headerlink" title="1、数据扩增"></a>1、数据扩增</h4><h4 id="2、early-stoping"><a href="#2、early-stoping" class="headerlink" title="2、early stoping"></a>2、early stoping</h4><pre><code>early stoping就是在中间点停止迭代过程，得到一个w中等大小的佛罗贝尼乌斯范数。
</code></pre><h2 id="过拟合-处理小结"><a href="#过拟合-处理小结" class="headerlink" title="过拟合 处理小结"></a>过拟合 处理小结</h2><pre><code>      机器学习过程中需要解决的两个问题：  
      1、代价函数优化  
      解决这一问题的工具：梯度下降、Momentum、RMSProp、Adam。  
      2、过拟合问题处理  
      正则化、扩增数据等  
</code></pre><script type="math/tex; mode=display">
</script><pre><code>        在优化代价函数时，只需要留意 $w 和 b, J(w,b)$ 值越小越好，过拟合问题即减少方差，使用另一套工具实现，这个原理被称为**正交化**。
        early stoping的主要 **缺点：** 是不能独立处理这两个问题，因为在提早停止了梯度下降也就结束优化代价函数； **优点：** 只运行一次梯度下降，就可以找出 $w$ 的较小值、中间值和较大值，而无需尝试 $L2$ 正则化超级参数的 $\lambda$ 的很多值。
</code></pre><h2 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h2><h3 id="1、归一化输入"><a href="#1、归一化输入" class="headerlink" title="1、归一化输入"></a>1、归一化输入</h3><script type="math/tex; mode=display">
</script><pre><code>      两个步骤：零均值、归一化方差  
      无论是训练集还是测试集都是通过相同的 $\mu\text和\sigma^&#123;2&#125;$ 定义的数据转换，这两个是由训练集得出的。  

        * 零均值化
</code></pre><script type="math/tex; mode=display">
向量
\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)} \\
x = x - \mu</script><pre><code>        * 归一化方差
</code></pre><script type="math/tex; mode=display">
数据离散程度：\sigma^{2} = \frac{1}{m}\sum_{i=1}^{m}(x^{(i)})^{2} \\
归一化：x = \frac{x}{\sigma^{2}}</script><h3 id="2、参数初始化"><a href="#2、参数初始化" class="headerlink" title="2、参数初始化"></a>2、参数初始化</h3><pre><code>      问题  

        梯度消失
        梯度爆炸  
</code></pre><p>训练神经网络的时候，有时候梯度会变得非常大或非常小，这造成了训练的难度（难以收敛）  </p>
<p>解决办法<br>权重初始化，设置权重矩阵：</p>
<script type="math/tex; mode=display">
w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}}) \\
$$ 激活函数为 tanh，使用： $\sqrt{\frac{1}{n^{[l-1]}}}$ ，也被称为Xavier初始化。  
激活函数为 Relu，使用： $\sqrt{\frac{2}{n^{[l-1]}}}$

## 梯度检查

      用于发现 backprop</script><pre><code>  \text 1）计算gradapprox \\
  1. \theta^&#123;+&#125; = \theta + \varepsilon\\  
  2. \theta^&#123;-&#125; = \theta - \varepsilon\\ 
  3. J^&#123;+&#125; = J(\theta^&#123;+&#125;)\\ 
  4. J^&#123;-&#125; = J(\theta^&#123;-&#125;)\\ 
  5. gradapprox = \frac&#123;J^&#123;+&#125; - J^&#123;-&#125;&#125;&#123;2  \varepsilon&#125; \\
  2) 通过反向传播，计算 grad  
  3) 计算梯度差值：  
</code></pre><script type="math/tex; mode=display">
</script><pre><code>  difference = \frac&#123;\mid\mid grad - gradapprox \mid\mid_2&#125;&#123;\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2&#125;   \tag&#123;2&#125;
</code></pre><script type="math/tex; mode=display">
      如果差值较小（比如小于： $10^{-7}$ ）则可判断梯度计算正确。  

## 优化算法

      目的：加快神经网络的运行。  
### （一）mini-batch梯度下降
#### 处理过程</script><pre><code>          \text 前向传播： Z^&#123;[1]&#125; = W^&#123;[1]&#125;X^&#123;\&#123;t\&#125;&#125; + b^&#123;[1]&#125;\\ 
          子集代价函数： J = \frac&#123;1&#125;&#123;num&#125;\sum_&#123;i=1&#125;^&#123;num&#125;L(\hat&#123;y&#125;^&#123;(i)&#125;, y^&#123;(i)&#125;)\\  
          正则化代价函数： J^&#123;\&#123;t\&#125;&#125; = \frac&#123;1&#125;&#123;num&#125;\sum_&#123;i=1&#125;^&#123;num&#125;L(\hat&#123;y&#125;^&#123;(i)&#125;, y^&#123;(i)&#125;) + \frac&#123;\lambda&#125;&#123;2\times&#123;1000&#125;&#125;\sum_&#123;l&#125;\mid\mid&#123;w^&#123;[l]&#125;\mid\mid&#125;^&#123;2&#125;_&#123;F&#125; \\ 
          参数更新： W^&#123;[l]&#125; = W^&#123;[l]&#125; - \alpha \times&#123;dW^&#123;[l]&#125;&#125;\\  
</code></pre><script type="math/tex; mode=display">

                * 训练时，每个mini-batch都会进行一次完整的前线传播、反向传播、参数更新。在一个epoch内，实际上已经进行了多次参数更新。

#### batch size 选择

              两个极端变种：  

                * mini-batch 大小等于m，其实就是batch梯度下降；
                * mini-batch 大小为1，则为随机梯度下降；  

通常选择2的n次方，如：64、256、512。

### （二）基于指数加权平均的优化算法</script><pre><code>      \text 指数加权平均数（指数加权移动平均数）：平均了 $\frac&#123;1&#125;&#123;1-\beta&#125;$ 个数据。  
</code></pre><script type="math/tex; mode=display">
</script><pre><code>          v_&#123;t&#125; = \beta \times&#123;v_&#123;t-1&#125;&#125; + (1-\beta) \times \theta _&#123;t&#125;
</code></pre><script type="math/tex; mode=display">
          偏差修正</script><pre><code>      v_&#123;t&#125; = \frac&#123;v_&#123;t&#125;&#125;&#123;1-\beta ^&#123;t&#125;&#125;
</code></pre><script type="math/tex; mode=display">
          下述的算法优化都是在正向传播、反向传播后得到参数的梯度值，在参数更新前对梯度值先进行平滑后再做参数更新。  
### 1、动量梯度下降（Momentum）</script><pre><code>      \begin&#123;cases&#125;
      v_&#123;dW^&#123;[l]&#125;&#125; = \beta v_&#123;dW^&#123;[l]&#125;&#125; + (1 - \beta) dW^&#123;[l]&#125; \\
      W^&#123;[l]&#125; = W^&#123;[l]&#125; - \alpha v_&#123;dW^&#123;[l]&#125;&#125; \\
      \end&#123;cases&#125;\tag&#123;3&#125; 
</code></pre><script type="math/tex; mode=display">
</script><pre><code>      \begin&#123;cases&#125;
      v_&#123;db^&#123;[l]&#125;&#125; = \beta v_&#123;db^&#123;[l]&#125;&#125; + (1 - \beta) db^&#123;[l]&#125; \\
      b^&#123;[l]&#125; = b^&#123;[l]&#125; - \alpha v_&#123;db^&#123;[l]&#125;&#125; \\
      \end&#123;cases&#125;\tag&#123;4&#125; \\
</code></pre><script type="math/tex; mode=display">
          where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate.  
### 2、RMSprop（Root Mean Square prop）</script><pre><code>      \begin&#123;cases&#125;
      s_&#123;dW^&#123;[l]&#125;&#125; = \beta_2 s_&#123;dW^&#123;[l]&#125;&#125; + (1 - \beta_2) (\frac&#123;\partial \mathcal&#123;J&#125; &#125;&#123;\partial W^&#123;[l]&#125; &#125;)^2 \\\
      W^&#123;[l]&#125; = W^&#123;[l]&#125; - \alpha \frac&#123;dW&#125;&#123;\sqrt&#123;s_&#123;dW^&#123;[l]&#125;&#125;&#125;&#125;\\\
      s_&#123;db^&#123;[l]&#125;&#125; = \beta_2 s_&#123;db^&#123;[l]&#125;&#125; + (1 - \beta_2) (\frac&#123;\partial \mathcal&#123;J&#125; &#125;&#123;\partial b^&#123;[l]&#125; &#125;)^2 \\\
      b^&#123;[l]&#125; = b^&#123;[l]&#125; - \alpha \frac&#123;db&#125;&#123;\sqrt&#123;s_&#123;db^&#123;[l]&#125;&#125;&#125;&#125;
      \end&#123;cases&#125;
</code></pre><script type="math/tex; mode=display">

### 3、Adam 优化算法
******</script><p>\begin{cases}<br>          v_{dW^{[l]}} &amp;= \beta_1 v_{dW^{[l]}} + (1 - \beta_1) (\frac{\partial \mathcal{J} }{\partial W^{[l]} }) \\\<br>          v^c_{dW^{[l]}} &amp;= \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\\<br>          s_{dW^{[l]}} &amp;= \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\\<br>          s^c_{dW^{[l]}} &amp;= \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\\<br>          W^{[l]} &amp;= W^{[l]} - \alpha (\frac{v^c_{dW^{[l]}}}{\sqrt{s^c_{dW^{[l]}}} + \varepsilon})<br>\end{cases}</p>
<script type="math/tex; mode=display">
</script><script type="math/tex; mode=display">
          where:          
            * t counts the number of steps taken of Adam
            * L is the number of layers
            * $\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages.
            * $\alpha$ is the learning rate
            * $\varepsilon$ is a very small number to avoid dividing by zero

## (三) 学习率 

      指数衰减公式：</script><pre><code>  \alpha = 0.95^&#123;epoch\_num&#125;  \alpha_0 
</code></pre><script type="math/tex; mode=display">
      其他衰减公式：</script><pre><code>  \alpha = \frac&#123;k&#125;&#123;\sqrt&#123;epoch\_num&#125;&#125; \alpha_0  \\
  \alpha = \frac&#123;k&#125;&#123;\sqrt&#123;t&#125;&#125; \alpha_0  \\
  \text (t为mini-batch数字）
</code></pre><script type="math/tex; mode=display">

## (四) 归一化网络激活函数(Batch Normalization)

        * 类似于输入X，把l-1层的激活值当作l层的输入，归一化也可加快训练速度。与X归一化也存在差别：隐藏单元不必是平均值0和方差1。
        * 隐藏层归一化选择在调用激活函数前，而归一化Z值，即： $a^{[l]} = g(\tilde{z}^{[l]})$ 。

### 隐藏层归一化公式</script><script type="math/tex; mode=display">
</script><pre><code>      \mu = \frac&#123;1&#125;&#123;m&#125;\sum_&#123;i=1&#125;^&#123;m&#125;z^&#123;[l](i)&#125; \\
      \sigma^&#123;2&#125; = \frac&#123;1&#125;&#123;m&#125;\sum_&#123;i=1&#125;^&#123;m&#125;(z^&#123;[l](i)&#125;-\mu)^&#123;2&#125; \\
      z^&#123;[l](i)&#125;_&#123;norm&#125; = \frac&#123;z^&#123;[l](i)&#125;-\mu&#125;&#123;\sqrt&#123;\sigma^&#123;2&#125;+\epsilon&#125;&#125; \\
      \text&#123;上面公式完成了归一化，但隐藏单元并不一定想平均值为0，方差为1，下面公式中新参数决定了均值和方差&#125; \\
      \tilde&#123;z&#125;^&#123;[l](i)&#125; = \gamma z^&#123;[l](i)&#125;_&#123;norm&#125; + \beta \\ 
</code></pre><script type="math/tex; mode=display">
          l层参数增加两个： $\beta 和 \gamma$ ，利用梯度下降进行优化：</script><pre><code>      \beta^&#123;[l]&#125; = \beta^&#123;[l]&#125; - \alpha d\beta^&#123;[l]&#125; \\
      \gamma^&#123;[l]&#125; = \gamma^&#123;[l]&#125; - \alpha d\gamma^&#123;[l]&#125;
</code></pre><script type="math/tex; mode=display">
### 归一化有效性原理

          1）类似于对输入进行归一化操作后加快梯度下降速度的原理，对隐藏层进行归一化操作有着相似的作用；  
          2）Batch 归一化减少了输入值改变的问题,它的确使这些值变得更稳定,神经网络的之后层就会有更坚实的基础。 即使使输入分布改变了一些, 它会改变得更少。 它做的是当前层保持学习,当改变时,迫使后层适应的程度减小了,你可以这样想,它减弱了前层参数的作用与后层参数的作用之间的联系, 它使得网络每层都可以自己学习, 稍稍独立于其它层, 这有助于加速整个网络的学习。  
### Batch 归一化的使用
#### 网络框架

        Tensorflow
                  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.batch_normalization()</span><br></pre></td></tr></table></figure>

#### 使用方法

        训练时的Batch Norm  
Batch归一化通常和训练集的mini-batch一起使用。  
按 $mini-batch(X^{[i]})$ 为单位计算 $z^{[l]}、\mu、\sigma^{2}$ 。  
        测试时的Batch Norm  
问题：单个样本的测试如何获得 $\mu 和 \sigma$ ?  
使用指数加权平均进行估算，这个平均数涵盖所有的mini-batch，计算过程：  
                1. 计算每个mini-batch的每个隐藏层计算 $\mu^{\{i\}[l]}、\sigma^{2\{i\}[l]}$ ;
                2. 使用指数加权平均计算训练过程中得到的 $\mu^{\{i\}[l]}、\sigma^{2\{i\}[l]}$
                3. 计算 $z^{ i}_{ norm}、~{z} $

## Softmax 回归

        * 线形变换及激活</script><p>Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\<br>t = e^{Z^{[l]}} \\<br>a^{[l]} = \frac{t_{i}}{\sum{_{i=1}^{n}}t_{i}}</p>
<script type="math/tex; mode=display">
        * 损失函数</script><p>L(\hat{y}, y) = -\sum_{j=1}^{n}y_{j}\log\hat{y}_{j}</p>
<script type="math/tex; mode=display">
        * 代价函数</script><p>J(w^{[l]}, b^{[l]}, … …) = \sum_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)})</p>
<script type="math/tex; mode=display">
        * softmax 与 hardmax  
Softmax 这个名称的来源是与所谓 hardmax 对比, hardmax 会把向量 z 变成这个向量[ 1 0 0 0 ], hardmax 函数会观察 z 的元素,然后在 z 中最大元素的位置放上 1,其它位置放上 0。  
        * 反向传播</script><p>\frac{\partial{L}}{\partial{z}} = dz^{[l]} = \hat{y} - y</p>
<p>$$</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/25/third-git-blog/" data-id="clkki76rx000758h7ao944w04" class="article-share-link" data-share="baidu" data-title="第二章 改善深层神经网络：超参数调试、正则化及优化">Share</a>
      

      
        <a href="http://example.com/2023/07/25/third-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/07/19/second-git-blog/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">第一章 神经网络和机器学习</div>
    </a>
  
</nav>

  
</article>


  <section id="comments">
    <div id="ds-thread" class="ds-thread" data-thread-key="2023/07/25/third-git-blog/" data-title="第二章 改善深层神经网络：超参数调试、正则化及优化" data-url="http://example.com/2023/07/25/third-git-blog/"></div>
  </section>
</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82/">参数调节</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" style="font-size: 10px;">深度挖掘</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">5</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/25/third-git-blog/">第二章 改善深层神经网络：超参数调试、正则化及优化</a>
          </li>
        
          <li>
            <a href="/2023/07/19/second-git-blog/">第一章 神经网络和机器学习</a>
          </li>
        
          <li>
            <a href="/2023/07/18/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2023/07/17/first-git-blog/">前言</a>
          </li>
        
          <li>
            <a href="/2023/07/17/my-first-blog/">Hexo基本使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://arvinxiang.com" target="_blank">主题作者</a>
          </li>
        
          <li>
            <a href="http://reqianduan.com" target="_blank">热前端</a>
          </li>
        
          <li>
            <a href="http://yuancheng.work" target="_blank">远程.work</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
