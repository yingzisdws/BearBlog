
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>熊熊随手记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="熊熊随手记">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="熊熊随手记">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="熊熊随手记" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">熊熊随手记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="example.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main">
  
    <article id="post-second-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/19/second-git-blog/" class="article-date">
  <time datetime="2023-07-19T08:36:59.000Z" itemprop="datePublished">2023-07-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/second-git-blog/">第一章</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="神经网络概要"><a href="#神经网络概要" class="headerlink" title="神经网络概要"></a>神经网络概要</h1><hr>
<p>首先吴恩达通过房价预测的例子，来形象的介绍了神经网络的作用。首先输入X，经过强大的神经网络，可将它映射成Y。<br>吴恩达在课程中也提到神经网络技术概念已经有几十年，为什么最近变得如此热门。笔者在05年研究生学习的时候，当时的课程已经是神经网络和机器学习，那个时候国内研究应用成果的还是比较少，但是理论的研究已经开始兴起，也是受国外很多期刊的影响，那个时候西安交大的徐宗本教授在此领域已经积累了不少经验和成果，后来凭借着这些成果，也当选成中科院院士，当时徐教授任笔者所在学院的院长，所以有幸也听过一些讲座。但随着后来没有从事相关领域的工作，到现在再重新拾起这门课，已经过了十几年的时光了。</p>
<p>回到刚才那个问题，为什么目前神经网络变得如此热门，吴恩达也提到这里面数据规模、计算量及算法的创新都功不可没。</p>
<h1 id="神经网络介绍"><a href="#神经网络介绍" class="headerlink" title="神经网络介绍"></a>神经网络介绍</h1><hr>
<p>为了使得神经网络的概念更好理解，吴恩达通过逻辑回归的例子来类比神经网络，使得我们通过一个简单的例子，了解神经网络的组成。以下图形很好的展示了神经网络的例子，此例子中的神经网络只包含了一个隐藏层。</p>
<p>  <img src="/2023/07/19/second-git-blog/1689757652003.png" alt="神经网络框架图"></p>
<p>  其中输入特征被竖直堆叠起来，叫做神经网络的输入层。另外一层称为隐藏层；最后一层只由一个结点构成，这个被称做输出层，它负责产生预测值。<br>  神经网络的三个要点描述为以下：</p>
<h3 id="输入（Input）-X"><a href="#输入（Input）-X" class="headerlink" title="输入（Input）:X"></a>输入（Input）:X</h3><h3 id="输出-（Output-Y"><a href="#输出-（Output-Y" class="headerlink" title="输出 （Output):Y"></a>输出 （Output):Y</h3><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><h3 id="激活函数-（Activation-Function）"><a href="#激活函数-（Activation-Function）" class="headerlink" title="激活函数 （Activation Function）"></a>激活函数 （Activation Function）</h3><pre><code>使用一个神经网络时，需要决定使用哪种激活函数用在隐藏层上，哪种用在输出节点上。文中介绍比较多的是sigmoid激活函数，实际应用中其他的激活函数效果会更好。激活函数实现了样本特征的非线形变换，如果没有非线形变换，可以证明即使有再多道隐层，神经网络也会“坍缩”成为逻辑回归。

常用激活函数：
</code></pre><ul>
<li><p>Tanh</p>
<script type="math/tex; mode=display">
g(z) = tanh(z)
      =\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\
      g'(z) = 1 -(tanh(z))^{2}</script></li>
<li><p>ReLU</p>
</li>
</ul>
<script type="math/tex; mode=display">
g(z) = max(0, z)
    g'(z) = 

    \begin{cases}
    0 & \text{if} \ z < 0  \\
    1 & \text{if} \ z ≥ 0
    \end{cases}</script><ul>
<li>Leaky ReLU</li>
</ul>
<script type="math/tex; mode=display">
g(z) = max(0.01, z) \\
g'(z) = \begin{cases}
0.01 & \text{ if}\ z \lt 0 \\
1 & \text{ if}\ z \ge 0
\end{cases}</script><ul>
<li>sigmoid</li>
</ul>
<script type="math/tex; mode=display">
g(z) = \frac{1}{1 + e^{-z}} \\
g'(z) = g(z) \times (1 - g(z))</script><ul>
<li>softmax</li>
</ul>
<script type="math/tex; mode=display">
\text{for } x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}
      x_1  &&
      x_2 &&
      ...  &&
      x_n  
    \end{bmatrix}) =
     \begin{bmatrix}
       \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &&
      \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &&
      ...  &&
      \frac{e^{x_n}}{\sum_{j}e^{x_j}} 
    \end{bmatrix}  \\
    \\</script><script type="math/tex; mode=display">
softmax(x) = softmax
    \begin{bmatrix}
      x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
      x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      x_{m1} & x_{m2} & x_{m3} & \dots  & x_{mn}
    \end{bmatrix} = 
    \begin{bmatrix}
      \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots  & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\
      \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots  & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots  & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}
    \end{bmatrix} \\
  = 
    \begin{pmatrix}
      softmax\text{(first row of x)}  \\
      softmax\text{(second row of x)} \\
      ...  \\
      softmax\text{(last row of x)} \\
    \end{pmatrix}</script><h3 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数 (Loss Function)"></a>损失函数 (Loss Function)</h3><pre><code>又叫误差函数，用来衡量算法的运行情况，通过这个L称为的损失函数，衡量预测输出值和实际值有多接近。一般用预测值和实际值的平方差或者它们平方差的一半。
              *   L1 损失函数
</code></pre><script type="math/tex; mode=display">
\begin{align*}         & L_1(\hat{y},y) = \sum_{i=0}^m|(y^{(i)} - \hat{y}^{(i)})|          \end{align*}</script><pre><code>*   L2 损失函数
</code></pre><script type="math/tex; mode=display">
\begin{align*}         & L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2          \end{align*}</script><pre><code>但通常逻辑回归中，会使用下面的L，
</code></pre><script type="math/tex; mode=display">
L(a, y) = -(ylog(a) + (1-y)log(1-a))
    L'(a,y) = \frac{1-y}{1-a} - \frac{y}{a}</script><pre><code>这个的好处，以及这门课很多函数也和这个原理类似，就是如果y等于1，尽可能让$\hat&#123;y&#125;$变大，
如果y等于0，尽可能让$\hat&#123;y&#125;$变小
</code></pre><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 (Cost Function)"></a>代价函数 (Cost Function)</h3><script type="math/tex; mode=display">
\frac{1}{m}\sum_{i=1}^{m}L(a^{(i)}, y^{(i)})</script><pre><code>损失函数()适用于单个训练样本，代价函数是参数的总代价
</code></pre><ul>
<li>参数()：$w$, $b$*</li>
</ul>
<h2 id="逻辑回归与神经网络"><a href="#逻辑回归与神经网络" class="headerlink" title="逻辑回归与神经网络"></a>逻辑回归与神经网络</h2><hr>
<p>关于逻辑回归和神经网络类比，有以下的结论：其中逻辑回归可以看作是单层（没有隐藏层）的二分类神经网络，其中<br>  输入特征向量X，预测目标为0或1<br>  所以逻辑回归是给出输入x以及参数w和b之后，如何输出预测值$\hat{y}$。</p>
<p>  神经网络可以从两个方面看作是逻辑回归的叠加：</p>
<ul>
<li>层数的叠加<br>神经网络有多个隐层，每层有自己独立的参数。</li>
<li><p>特征参数的叠加<br>神经网络每一层在传入各层激活函数前，会进行多种不同的线形变换，每种变换经激活函数计算后形成该层一个神经元，一个神经元对应一次逻辑回归计算，同时也对应了参数矩阵一行。</p>
<p>以下是关于逻辑回归的训练目标与训练过程的介绍</p>
</li>
</ul>
<h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><hr>
<p>  逻辑回归作为二分类算法，其损失函数的数学意义可以理解为预测值$\hat{y}$为真实值$y$的概率，即：</p>
<script type="math/tex; mode=display">
y = 1: p(y|x) =\hat{y}y = 0: p(y|x) = 1 - \hat{y}</script><pre><code>将两个公式合并为一个公式：
</code></pre><script type="math/tex; mode=display">
p(y|x) = \hat{y}^{y}(1-\hat{y})^{(1-y)}</script><pre><code>基于上述公式，我们需要 p(y|x) 所得概率值越大越好(从而达到预测值与真实值逼近的目的)。为避免进行指数运算所带来的复杂性，利用对数函数的严格单调性，对上述公式两边同时取对数，转换为下面的公式：
</code></pre><script type="math/tex; mode=display">
\log{p(y|x)} = y\log{\hat{y}} + (1-y)\log(1-\hat{y})
    = -L(\hat{y}, y)</script><pre><code>概率值越大，损失值应该越小，二者应是相反关系且概率值都是小于1，其对数为负，所以损失值取负。
训练的目标就是通过寻找参数$w$, $b$,以获得最小的损失值。每次迭代时参数更新公式为：
</code></pre><script type="math/tex; mode=display">
w = w - \alpha\times dw
    b = b - \alpha\times db</script><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><hr>
<p>   寻找最优参数过程基于凸函数的梯度下降()原理，通过对训练集迭代进行“正向传播()”和“反向传播()”完成。</p>
<pre><code>#### 正向传播
根据样本数据$x$推导出$hat&#123;y&#125; $,并保留中间值$z$用于反向传播的计算；
#### 反向传播
</code></pre><script type="math/tex; mode=display">
\frac{da}{dz} = a \times(1 - a)\\
    da = \frac{dL}{da} = \frac{1-y}{1-a} - \frac{y}{a} \\
    dz =\frac{dL}{dz} = \frac{dL}{da} \times \frac{da}{dz} = a - y\\

    \frac{dz}{dw} = x\\
    \frac{dz}{db} = 1 \\
    dw = \frac{dL}{dw} = \frac{dL}{dz} \times \frac{dz}{dw} = (a-y) \times x \\
    db = \frac{dL}{db} = \frac{dL}{dz} \times \frac{dz}{db} = a - y</script><ul>
<li>多样本向量化计算<br>使用向量化计算比循环运算快约400倍。<br>输入为一个$（n，m）$的矩阵，m列的列向量，每个样本为$n$个特征值；<br>参数$w$维度为：（1，n),参数$b$维度为：（1，1)</li>
</ul>
<h2 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h2><h3 id="向量化运算"><a href="#向量化运算" class="headerlink" title="向量化运算"></a>向量化运算</h3><p>  向量在网络各层计算过程中的维度规律：</p>
<pre><code>* 各层样本个数保持不变，即：$A^&#123;[i]&#125;$、$Z^&#123;[i]&#125;$ 的列值均保持为m(样本数)，变化的是各层每个样本的特征数(行数)
* 参数 $W^&#123;[i]&#125;$的维度与样本数无关，行为本层需生成的样本特征数，列为上层特征数；$B^&#123;[i]&#125;$为本层特征数，一列
* 对于反向传播时的导数向量同样如此。
</code></pre><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><pre><code>#### 逻辑回归 参数初始化
  不存在中间隐层，输出层也只有一个神经元，参数可以初始化为任意值，考虑到激活函数曲线的斜率特性，通常都初始化为0。
#### 神经网络 参数初始化
  神经网络因隐层存在多个神经元，如参数初始化为0，则可证明每个神经元作用是等价的，因此，神经网络参数需初始化为随机值，且乘以一个较小的数，如：0.01，以使计算从斜率较大的地方开始。

* 另一项AI通用技术-数据归一化：训练数据归一化后将提高梯度下降的性能。举例, 如果：
</code></pre><script type="math/tex; mode=display">
x = \begin{bmatrix}
      0 & 3 & 4\\
      2 & 6 & 4 \\
      \end{bmatrix}\\

    \text{then}\\ \| x\| = np.linalg.norm(x, axis = 1, keepdims = True)

    = \begin{bmatrix}
      5\\
      \sqrt{56} \\
      \end{bmatrix}

    x\_normalized = \frac{x}{\| x\|} =
      \begin{bmatrix}
      0 & \frac{3}{5} & \frac{4}{5} \\
      \frac{2}{\sqrt{56}} & \frac{6}{\sqrt{56}} & \frac{4}{\sqrt{56}} \\
      \end{bmatrix}</script><h3 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h3><p>  上述介绍了神经网络的训练目标和训练过程，接下来我们来介绍，为什么要使用深层神经网络？<br>    过去实践经验，有些函数，只有非常深的神经网络能学会，而浅的模型办不到。<br>    同样课堂上将深度神经网络和人类大脑相比较，即先探测简单的东西（开始的层获取底层特征），组合起来才能探测复杂的物体（底层特征组合后的特征），从功能上讲，浅层网络也能达到同样效果，但是计算的效率较低。<br>    有两个特点<br>    small: 隐藏单元的数量相对较少<br>    deep: 隐藏层数目比较多<br>    深层网络隐藏单元数量相对较少，隐藏层数较多。如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<ul>
<li>参数      超参数：<br>算法中的学习率：learning rate $\alpha$<br>梯度下降循环的数量:n<br>隐藏层数目：L<br>隐藏层单元数目(): $n^[l]$<br>激活函数的选择<br>需要操作者自行设置的，这些数字实际上控制了最后的参数$w$, $b$，称之为超参数<br>按下来第二门课将会详细介绍参数的选择、正则化以及优化。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/19/second-git-blog/" data-id="clkhqscxe000318h7f8sq5tf0" class="article-share-link" data-share="baidu" data-title="第一章">Share</a>
      

      
        <a href="http://example.com/2023/07/19/second-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/18/hello-world/" class="article-date">
  <time datetime="2023-07-18T03:54:17.414Z" itemprop="datePublished">2023-07-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/18/hello-world/" data-id="clkhqscx7000018h70y0uc4w6" class="article-share-link" data-share="baidu" data-title="Hello World">Share</a>
      

      
        <a href="http://example.com/2023/07/18/hello-world/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  
    <article id="post-first-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/17/first-git-blog/" class="article-date">
  <time datetime="2023-07-17T08:36:59.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/first-git-blog/">前言</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="写在博客前面"><a href="#写在博客前面" class="headerlink" title="写在博客前面"></a><strong>写在博客前面</strong></h1><hr>
<p>前后差不多将近一年半的时间（从2021年的10月份从网上看到深度学习的经典之作开始学习），比较曲折，耗时较长，中间还经历过上海封控等因素（2022.3-6），同时大家因为也有正常的工作要做，留给深度学习的时间比较零碎，但其实去年的时候（2022.5），已经完成了主体部分的学习。后来计划是写些东西，首先要整理笔记，把之前各门科的笔记和对应作业的要点整理起来，差不多在今年过年时候完成（2023.2）。然后这个写作的任务就落在我的头上，加上今年公司的改革，我的工作上半年特别忙碌，所以笔记拖到现(2023.5)在。最近是两个项目的空档时间，抓紧利用这个时间可以快速把之前的学习经历、心得和作业要点通通梳理，也算是为这次学习画上完满的句号。之后将是针对所学内容的开发工具或者相关算法的改进，这个是后话，也希望学到的东西可以持续的应用起来。</p>
<h1 id="为什么选择这套教材"><a href="#为什么选择这套教材" class="headerlink" title="为什么选择这套教材"></a><strong>为什么选择这套教材</strong></h1><hr>
<p>深度挖掘是当今科技界比较热门的技能，对于想入门的同学，除了要掌握一般的编程知识，熟悉机器学习的基本概念，还需要有python编程的基础。但即使掌握了这些技能，如何选择一门好的入门课程仍然困扰着大家。例如笔者一开始的时候，网上购买了一些付费课程，但是网上的课程鱼龙混杂，跟着视频学习的时候，发现遇到了更多的问题，另外课程上存在着各种只是简单的复制，没有注重课程的连续性，或者是其中一些必要的知识，没有介绍清楚，导致学习起来特别痛苦。花了很多时间去调查中间的过程。<br>在中间探索的过程中，发现了吴恩达YYDS的深度挖掘的课程，但是由于笔者和朋友已经不接触英语好久了，对于全英文的课程，心里还是存着敬畏。感觉没有勇气能够坚持完全部课程的学习。后来发现了已经翻译成中文的学习笔记和网易云课程已经将视频做了翻译，这样就扫清了前期学习的语言障碍。<br>吴恩达这些课程，总共有5堂课，通过课程的学习，可以学习到深度学习的基础，学会构建神经网络。并且可以在吴恩达本人及多位业务顶尖专家的指导下创建自己的机器 学习项目。其中对卷积神经网络(CNN)、递归神经网络（RNN）、长短期记忆（LSTM）等深度学习都有所涉及。每节课都配有习题和编程练习，之后我们将会分章节，解析习题和编程练习，以便大家可以更好地了解深度挖掘的原理和应用。</p>
<h1 id="章节内容介绍"><a href="#章节内容介绍" class="headerlink" title="章节内容介绍"></a><strong>章节内容介绍</strong></h1><hr>
<p>主要是包括五门课的内容，主要的内容和结构如下:<br>文章的脉落也比较清晰，第一门课是对于神经网络用到的基础进行阐述，同时介绍了浅层网络和深层网络的基础，在大家对于这些概念有些基本了解后，开始进行深入探索。在第二门课时，对于深层神经网络的参数调节、正则化以及优化做了详细的阐述，紧接着是第三门课对于结构化机器学习项目的学习策略进行介绍，主要是基于作者在日常的项目中遇到的问题以及大家经常关心的策略等有了比较详细的介绍。之后在第四门课时介绍了卷积神经网络，以及目标检测的应用，通过两个特殊应用：人脸识别和神经风格的转换，介绍了卷积神经网络的应用。最后一门课第五门课是关于序列模型的介绍，这块对于目前自然语言的应用等领域有着非常好的借鉴作用。</p>
<h2 id="章节列表"><a href="#章节列表" class="headerlink" title="章节列表"></a><strong>章节列表</strong></h2><hr>
<h3 id="第一门课-神经网络和深度学习"><a href="#第一门课-神经网络和深度学习" class="headerlink" title="第一门课  神经网络和深度学习"></a>第一门课  神经网络和深度学习</h3><pre><code>- 深度学习引言
- 神经网络的编程基础
- 浅层神经网络
- 深层神经网络
</code></pre><h3 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化" class="headerlink" title="第二门课  改善深层神经网络：超参数调试、正则化以及优化"></a>第二门课  改善深层神经网络：超参数调试、正则化以及优化</h3><pre><code>- 深度学习的实践层面
- 优化算法
- 超参数调试、Batch正则化和程序框架
</code></pre><h3 id="第三门课-结构化机器学习项目"><a href="#第三门课-结构化机器学习项目" class="headerlink" title="第三门课 结构化机器学习项目"></a>第三门课 结构化机器学习项目</h3><pre><code>- 机器学习策略1
- 机器学习策略2
</code></pre><h3 id="第四门课-卷积神经网络"><a href="#第四门课-卷积神经网络" class="headerlink" title="第四门课  卷积神经网络"></a>第四门课  卷积神经网络</h3><pre><code>- 卷积神经网络
- 深度卷积网络
- 目标检测
- 特殊应用：人脸识别和神经风格转换
</code></pre><h3 id="第五门课-序列模型"><a href="#第五门课-序列模型" class="headerlink" title="第五门课  序列模型"></a>第五门课  序列模型</h3><pre><code>- 循环序列模型
- 自然语言处理与词嵌入
- 序列模型和注意力机制
</code></pre>
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/17/first-git-blog/" data-id="clkhqscxc000118h7h33safy0" class="article-share-link" data-share="baidu" data-title="前言">Share</a>
      

      
        <a href="http://example.com/2023/07/17/first-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  
    <article id="post-my-first-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/17/my-first-blog/" class="article-date">
  <time datetime="2023-07-17T07:00:58.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/my-first-blog/">Hexo基本使用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hexo的优点：快速、简洁且高效的博客框架。Hexo使用Markdown解析文章，在几秒内，即可利用按照指定主题生成静态网页。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装使用hexo之前需要先安装Node.js和Git，当已经安装了Node.js和npm(npm是node.js的包管理工具)，可以通过以下命令安装hexo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<p>可以通过以下命令查看主机中是否安装了node.js和npm</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ node --version    <span class="comment">#检查是否安装了node.js</span></span><br><span class="line">$ npm --version     <span class="comment">#检查是否安装了npm</span></span><br></pre></td></tr></table></figure>
<p>如果已经安装，提示如下：</p>
<p>$ C:\Users\YingZhou\blog&gt;node —version<br>v18.16.1</p>
<p>C:\Users\YingZhou\blog&gt;npm —version<br>9.5.1</p>
<h1 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h1><p>安装完Hexo之后，执行下列命令，Hexo将会在指定目录中新建所需要的文件，指定的目录即为Hexo的工作站</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/17/my-first-blog/" data-id="clkhqscxe000218h78i726mt8" class="article-share-link" data-share="baidu" data-title="Hexo基本使用">Share</a>
      

      
        <a href="http://example.com/2023/07/17/my-first-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  

</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/19/second-git-blog/">第一章</a>
          </li>
        
          <li>
            <a href="/2023/07/18/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2023/07/17/first-git-blog/">前言</a>
          </li>
        
          <li>
            <a href="/2023/07/17/my-first-blog/">Hexo基本使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://arvinxiang.com" target="_blank">主题作者</a>
          </li>
        
          <li>
            <a href="http://reqianduan.com" target="_blank">热前端</a>
          </li>
        
          <li>
            <a href="http://yuancheng.work" target="_blank">远程.work</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
