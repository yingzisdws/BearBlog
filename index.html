
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>熊妞随手记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="熊妞随手记">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="熊妞随手记">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="熊妞随手记" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">熊妞随手记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="example.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main">
  
    <article id="post-fifth-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/08/15/fifth-git-blog/" class="article-date">
  <time datetime="2023-08-15T08:05:58.000Z" itemprop="datePublished">2023-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/fifth-git-blog/">第四章：卷积神经网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、卷积神经网络"><a href="#一、卷积神经网络" class="headerlink" title="一、卷积神经网络"></a>一、卷积神经网络</h1><p>  作者提到深度学习有两点让其兴奋，一是计算机视觉的高度发展标志着新型应用产生的可能。通过学习使用这些工具，可能会创造出新的产品和应用。其次，以计算机视觉举例，可以将其衍生的新的神经网络结构与算法，应用到其他领域的交叉成果。对计算机视觉来说，除了处理小图外，还需要处理大图，大图的话，就会用到卷积计算。作者通过边缘检测介绍了卷积是如何运算的，包括垂直边缘检测，如何区分正边和正边，以达到亮到暗的区别，即边缘的过渡。<br>  以下开始介绍卷积神经网络，定义以及应用；  </p>
<h2 id="1、定义"><a href="#1、定义" class="headerlink" title="1、定义"></a>1、定义</h2><pre><code>卷积网络的组成：卷积层、池化层、全连接层  </code></pre><h3 id="1）卷积"><a href="#1）卷积" class="headerlink" title="1）卷积"></a>1）卷积</h3><pre><code>* 功能  </code></pre><p>获取特征<br>            * 输入<br>$ N_{ H}^{ [l]} × N_{ W}^{ [l]} × N_{ C}^{ [l]} $<br>            * 运算<br>卷积核： $f \times f \times c$<br>Padding: $p$<br>              <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对 a 的 2nd，4th进行pad</span></span><br><span class="line">np.pad(a, ((<span class="number">0</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">0</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">&#x27;constant&#x27;</span>, constant_values=(..,..))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积操作：</span></span><br><span class="line">s = np.multiply(a_slice_prev, W) + b</span><br><span class="line">Z = np.<span class="built_in">sum</span>(s)</span><br></pre></td></tr></table></figure><br>步长： $s$<br>          *   输出<br>            $N^{[l]} = \frac{N^{[l-1]}+ 2 \times p^{[l]} - f^{[l]}}{s^{[l]}} + 1$<br>            1）Valid 卷积<br>            $p = 0$<br>            2）Same 卷积<br>            卷积前后的维度不变。<br>            $p = \frac{f-1}{2}$ ( step = 1)  </p>
<pre><code>* 训练参数个数  </code></pre><p>$f^{[l]} \times f^{[l]} \times c^{[l-1]} \times c^{[l]} + c^{[l]}$<br>            * 超参数<br>过滤器大小<br>步幅<br>Padding<br>过滤器个数</p>
<h4 id="互相关（cross-correlation）和卷积-convolution"><a href="#互相关（cross-correlation）和卷积-convolution" class="headerlink" title="互相关（cross-correlation）和卷积(convolution)"></a>互相关（cross-correlation）和卷积(convolution)</h4><pre><code>* 数学意义上或信号处理的卷积操作应还包含“翻转”操作，使得卷积运算具有下述数学性质(结合律)： $(A*B)*C=A*(B*C)$ ；
* 机器学习中的卷积操作则是省略了“翻转”操作，这一操作在数学上称为“互相关”；</code></pre><h3 id="2）池化"><a href="#2）池化" class="headerlink" title="2）池化"></a>2）池化</h3><pre><code>* 功能  </code></pre><p>缩简模型大小；提高计算速度；提高所获取特征的鲁棒性。<br>            * 运算<br><strong>最大池化（Max Pooling）：</strong> 如果在过滤器中提取到某个特征,那么保留其最大值。 如果没有提取到这个特征, 可能在右上象限中不存在这个特征, 那么其中的最大值也还是很小。<br>              <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure><br><strong>平均池化（Average Pooling）：</strong><br>              <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.mean()</span><br></pre></td></tr></table></figure><br>            * 训练参数：无（池化只是计算神经网络某一层的静态属性,没有什么需要学习的,它只是一个静态属性。）<br>            * 超参数： $f、s、p（池化很少用padding）$<br>            * 输出： $\frac{n + 2p -f}{s} + 1$ ; 通道数不变</p>
<h3 id="3）全连接"><a href="#3）全连接" class="headerlink" title="3）全连接"></a>3）全连接</h3><pre><code>类似于单神经网络层。  </code></pre><h3 id="4）卷积层"><a href="#4）卷积层" class="headerlink" title="4）卷积层"></a>4）卷积层</h3><pre><code>在计算神经网络有多少层时, 通常只统计具有权重和参数的层。 因为池化层没有权重和参数, 只有一些超参数。 这里, 我们把 CONV1 和 POOL1 共同作为一个卷积,并标记为 Layer1。  </code></pre><h2 id="2、训练"><a href="#2、训练" class="headerlink" title="2、训练"></a>2、训练</h2><pre><code>* 尽量不要自己设置超参数,而是查看文献中别人采用了哪些超参数,选一个在别人任务中效果很好的架构,那么它也有可能适用于你自己的应用程序。
* **常见模式：** 一个或多个卷积后面跟随一个池化层,然后一个或多个卷积层后面再跟一个池化层,然后是几个全连接层,最后是一个softmax。
* 随着神经网络计算深度不断加深, 通常开始时的图像也要更大一些， 然后随着网络深度的加深而逐渐减小而通道数在增加。</code></pre><h2 id="3、特点"><a href="#3、特点" class="headerlink" title="3、特点"></a>3、特点</h2><pre><code>较标准神经网络使用少得多的参数，主要原因有以下两点：  </code></pre><h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><pre><code>卷积核的平移使不同位置的类似特征使用相同的参数进行检测。  </code></pre><h3 id="稀疏连接"><a href="#稀疏连接" class="headerlink" title="稀疏连接"></a>稀疏连接</h3><pre><code>卷积网络第N层的某个特征值仅与N-1层中对应为止的 $f\times f$ 个元素相关，而不是全部元素。  </code></pre><h1 id="二、经典神经网络"><a href="#二、经典神经网络" class="headerlink" title="二、经典神经网络"></a>二、经典神经网络</h1><h2 id="1、研究目的"><a href="#1、研究目的" class="headerlink" title="1、研究目的"></a>1、研究目的</h2><pre><code>组件：卷积层、池化层以及全连接层。  
计算机视觉研究集中在如何把这些基本构件组合起来,形成有效的卷积神经网络。  
通过研究经典网络学习网络构建。  </code></pre><h2 id="2、经典网络"><a href="#2、经典网络" class="headerlink" title="2、经典网络"></a>2、经典网络</h2><pre><code>* LeNet-5
* AlexNet
* VGG
* ResNet
* Inception</code></pre><h3 id="1）LeNet-5"><a href="#1）LeNet-5" class="headerlink" title="1）LeNet-5"></a>1）LeNet-5</h3><pre><code>针对灰度图片进行训练。  

| 层 | 参数 | 维度 |
|---|---|---|
| 输入 | | 32 * 32 * 1 |
| 卷积 | 5 * 5 * 1 n=6,s=1,p=0 | 28 * 28 * 6 |
| 平均池化 | 2 * 2 s=2 | 14 * 14 * 6 |
| 卷积 | 5 * 5 * 1 n=16,s=1,p=0 | 10 * 10 * 16 |
| 平均池化 | 2 * 2 s=2 | 5 * 5 * 16 |
| 全连接 | 120 * 400 | 120 * 1 |
| 全连接 | 84 * 120 | 84 * 1 |
| softmax | 84 * 10 | 10 * 1 |

特点  

  * 6万多个参数
  * 随着网络层数的加深，图像电宽度和高度减小，通道数量则增加。</code></pre><h3 id="2）AlexNet"><a href="#2）AlexNet" class="headerlink" title="2）AlexNet"></a>2）AlexNet</h3><pre><code>  | 层 | 参数 | 维度 |
  |---|---|---|
  | 输入 | | 227 * 227 * 3 |
  | 卷积 | 11 * 11 * 3 n=96,s=4,p=0 | 55 * 55 * 96 |
  | 平均池化 | 3 * 3 s=2 | 27 * 27 * 96 |
  | 卷积 | 5 * 5 * 96 n=276,s=1,p=2 | 27 * 27 * 276 |
  | 最大池化 | 3 * 3 s=2 | 13 * 13 * 276 |
  | same卷积 | 3 * 3 * 276 n=384,s=1,p=1 | 13 * 13 * 384 |
  | same卷积 | 3 * 3 * 384 n=384,s=1,p=1 | 13 * 13 * 384 |
  | same卷积 | 3 * 3 * 384 n=256,s=1,p=1 | 13 * 13 * 256 |
  | 最大池化 | 3 * 3 s=2 | 6 * 6 * 256 |
  | 全连接 | 4096 * 9216 | 4096 * 1 |
  | 全连接 | 4096 * 4096 | 4096 * 1 |
  | softmax | 1000 * 4096 | 1000 * 1 |

  特点：  

    * 6000万个参数；
    * 能够处理非常相似的基本构造函数；
    * 使用ReLu函数；

- ### 3）VGG-16

  | 层 | 参数 | 维度 |
  |---|---|---|
  | 输入 | | 224 * 224 * 3 |
  | 2 * 卷积 | 3 * 3 * 3 n=64,s=1,p=same | 224 * 224 * 64 |
  | 最大池化 | 2 * 2 s=2 | 112 * 112 * 64 |
  | 2 * 卷积 | 3 * 3 * 3 n=128,s=1,p=same | 112 * 112 * 128 |
  | 最大池化 | 2 * 2 s=2 | 56 * 56 * 128 |
  | 3 * 卷积 | 3 * 3 * 3 n=256,s=1,p=same | 56 * 56 * 256 |
  | 最大池化 | 2 * 2 s=2 | 28 * 28 * 256 |
  | 3 * 卷积 | 3 * 3 * 3 n=512,s=1,p=same | 28 * 28 * 512 |
  | 最大池化 | 2 * 2 s=2 | 14 * 14 * 512 |
  | 3 * 卷积 | 3 * 3 * 3 n= 512,s=1,p=same | 14 * 14 * 512 |
  | 最大池化 | 2 * 2 s=2 | 7 * 7 * 512 |
  | 全连接 | 4096 * 25088 | 4096 * 1 |
  | 全连接 | 4096 * 4096 | 4096 * 1 |
  | softmax | 1000 * 4096 | 1000 * 1 |

  特点：  

    * 由16个卷积层、全连接层组成；
    * 网络结构简单，都是卷积后跟着压缩图像大小（缩小图像的宽度、高度）的池化层组成；
    * 卷积时进行过滤器翻倍；
    * 总共包含越1.38亿个参数，训练数据量大是其缺点；</code></pre><h3 id="4）残差网络"><a href="#4）残差网络" class="headerlink" title="4）残差网络"></a>4）残差网络</h3><pre><code>处理非常非常深的神经网络训练时所需面对的 **梯度消失** 和 **梯度爆炸** 问题。  
**残差块：** 通过跳跃连接的方式从某一层获取的激活值，反馈给另一层。  </code></pre><h4 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h4><p>数学推导：  </p>
<p>$$<br>              a^{[l]} \<br>              z^{[l+1]} = W^{[l+1]} a^{[l]} + b^{[l+1]} \<br>              a^{[l+1]} = g(z^{[l+1]}) \<br>              z^{[l+2]} = W^{[l+2]} a^{[l+1]} + b^{[l+2]} \<br>              原始连接：a^{[l+2]} = g(z^{[l+2]}) \<br>              跳跃连接：a^{[l+2]} = g(z^{[l+2]} + a^{[l]})<br>$$</p>
<pre><code>指$a^&#123;[l]&#125;$跳过一层或者好几层，从而将信息传递到神经网络的更深层。  </code></pre><h4 id="“残差”功能"><a href="#“残差”功能" class="headerlink" title="“残差”功能"></a>“残差”功能</h4><pre><code>* 使用标准优化算法训练一个普通网络,比如说梯度下降法,或者其它热门的优化算法。 如果没有残差,没有这些捷径或者跳跃连接,凭经验你会发现随着网络深度的加深, 训练错误会先减少, 然后增多。
* 使用 ResNets 即使网络再深, 训练的表现却不错, 比如说训练误差减少, 就算是训练深达 100 层的网络也不例外。</code></pre><h4 id="“残差”原理"><a href="#“残差”原理" class="headerlink" title="“残差”原理"></a>“残差”原理</h4><pre><code>* 效率不逊色于更简单的神经网络</code></pre><p>$$<br>跳跃连接：a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) \<br>a^{[l+2]} = g(W^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]})<br>$$ 如果使用L2正则化或权重衰减，训练过程中 $W^{[l+2]}$ 值会被压缩，对b同样如此。如果 $W^{[l+2]} 和 b$ 的值为0，则有 $a^{[l+2]} = g(a^{[l]}) = a^{[l]}（使用ReLu激活函数）$ 。残差块学习这个恒等式并不难，这意味着在网络中加入这个<strong>跳跃连接对效率影响有限</strong>。<br>                * 所增加的隐藏单元能学到一些有用的信息，这可能比学习恒等函数表现得更好。</p>
<h3 id="5）谷歌Inception网络"><a href="#5）谷歌Inception网络" class="headerlink" title="5）谷歌Inception网络"></a>5）谷歌Inception网络</h3><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><pre><code>Inception 网络或 Inception层的作用就是代替人工来确定卷积层中的过滤器类型, 或者确定是否需要创建卷积层或池化层。  </code></pre><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><pre><code>给网络添加这些参数的所有可能值,然后把这些输出连接起来,让网络自己学习它需要什么样的参数,采用哪些过滤器组合。  </code></pre><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><pre><code>**计算成本**  
使用1x1卷积，形成“瓶颈层”，先缩小网络表示，再扩大，从而达到减小计算成本的目的。  </code></pre><h2 id="3、网络中的网络以及-1X1-卷积"><a href="#3、网络中的网络以及-1X1-卷积" class="headerlink" title="3、网络中的网络以及 1X1 卷积"></a>3、网络中的网络以及 1X1 卷积</h2><pre><code>1 X 1卷积可以理解为一个对一个通道的全连接操作，并在 W x H 所有元素上进行计算。  

**功能**：可以**改变**或**不变**通道数的同时为神经网络添加一个非线形函数。  </code></pre><h2 id="4、方法论"><a href="#4、方法论" class="headerlink" title="4、方法论"></a>4、方法论</h2><h3 id="1）迁移学习"><a href="#1）迁移学习" class="headerlink" title="1）迁移学习"></a>1）迁移学习</h3><pre><code>用迁移学习把公共数据集上的知识迁移到你自己的问题上。  
“冻结”前面层的参数，把前面的层看作一个固定函数。  </code></pre><h3 id="2）数据增强"><a href="#2）数据增强" class="headerlink" title="2）数据增强"></a>2）数据增强</h3><pre><code>* 垂直镜像对称
* 随机裁剪
* 旋转、变形等
* 彩色转换</code></pre><h2 id="5、计算机视觉现状"><a href="#5、计算机视觉现状" class="headerlink" title="5、计算机视觉现状"></a>5、计算机视觉现状</h2><pre><code>数据 vs 人工处理  
机器学习算法有两种知识来源：  

  * 一个来源是被标记的数据,就像(x,y)应用在监督学习。
  * 第二个知识来源是手工工程,有很多方法去建立一个手工工程系统,它可以是源于精心设计的特征,手工精心设计的网络体系结构或者是系统的其他组件。</code></pre><h1 id="三、目标检测"><a href="#三、目标检测" class="headerlink" title="三、目标检测"></a>三、目标检测</h1><p>  三种类型的问题：  </p>
<pre><code>* 对象分类
* 定位分类
* 对象检测  </code></pre><p>前两个问题，通常只有一个较大等对象位于图像等中间位置，而对象检测则是图片中有多个对象。</p>
<h2 id="1、定位分类"><a href="#1、定位分类" class="headerlink" title="1、定位分类"></a>1、定位分类</h2><pre><code>分类问题通过常规的分类算法实现，定位问题则需重新定义目标标签。  </code></pre><h3 id="对象标签"><a href="#对象标签" class="headerlink" title="对象标签"></a>对象标签</h3><pre><code>8个分量参数表示。  </code></pre><p>$$<br>y = \begin{bmatrix}<br>        P_{c}  \<br>        b_{x} \<br>        b_{y} \<br>        b_{h} \<br>        b_{w} \<br>        c_{1} \<br>        c_{2} \<br>        c_{3}<br>\end{bmatrix}<br>$$<br>$P_{c}$ = 0 时，y的其他参数变得无意义。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><pre><code>1）平方误差策略（简化策略）：  </code></pre><ul>
<li>当 $y_{1}=1$ 时<br>$$<br>L(\hat{y}, y) = (\hat{y}_{1}-y_{1})^{2} + (\hat{y}_{2}-y_{2})^{2} + …(\hat{y}_{8}-y_{8})^{2}\tag{1}<br>$$</li>
<li>当 $y_{1}=0$ 时<br>$$<br>L(\hat{y}, y) = (\hat{y}_{1}-y_{1})^{2}<br>$$</li>
</ul>
<p>2）常规策略<br>$$<br>$$<br>            * 不对 $c_{1},c_{2},c_{3}$ 应用对数损失函数；<br>            * 对边界框坐标应用平方差或类似方法；<br>            * 对 $P_{c}$ 应用逻辑回归函数；</p>
<h3 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h3><pre><code>神经网络通过输出图片上对特征点的 $(x, y)$ 坐标来实现对目标特征的识别。  

标签定义(64个特征点)  </code></pre><p>$$<br>      y = \begin{bmatrix}<br>        P_{c}  \<br>        1_{1x}, 1_{1y} \<br>        1_{2x}, 1_{2y}  \<br>        … \<br>        1_{64x}, 1_{64y}  \<br>        \end{bmatrix} \<br>$$<br>        $P_{c}$ = 0 时，y的其他参数变得无意义。</p>
<pre><code>相同序号的特征点在所有图片中保持一致。  </code></pre><h2 id="2、对象检测"><a href="#2、对象检测" class="headerlink" title="2、对象检测"></a>2、对象检测</h2><h3 id="滑动窗口目标检测"><a href="#滑动窗口目标检测" class="headerlink" title="滑动窗口目标检测"></a>滑动窗口目标检测</h3><pre><code>**训练步骤：**  
1）裁剪图片（使对象占据整个图片），创建一个标签训练集；  
2）训练卷积网络，输出分类结果；  
3）使用不同大小的窗口进行滑动窗口目标检测，得到各个窗口的分类结果；  

**缺点：** 计算成本较高  </code></pre><h3 id="滑动窗口的卷机实现"><a href="#滑动窗口的卷机实现" class="headerlink" title="滑动窗口的卷机实现"></a>滑动窗口的卷机实现</h3><pre><code>**原理：**  
该卷积操作的原理是我们不需要把输入图像分割成四个子集（例：16x16图片传入14x14输入的卷积网络）,分别执行前向传播,而是把它们作为一张图片输入给卷积网络进行计算,其中的公共区域可以共享很多计算,就像这里我们看到的这个 4 个 14×14 的方块一样。  

这是一个逻辑上“嵌套”的卷积，卷积网络输入层作为卷积核。全连接层的卷积变换。  

**缺点：** 边界框的位置不够准确。  </code></pre><h2 id="3、边界框预测"><a href="#3、边界框预测" class="headerlink" title="3、边界框预测"></a>3、边界框预测</h2><pre><code>其中一个能更精准边界框的算法是Yolo（You Only Look Once）  
**步骤：**  
1）在图像上定义一个网格，如：3 x 3；  
2）定义网格的标签：  </code></pre><p>$$<br>      y = \begin{bmatrix}<br>        P_{c}  \<br>        b_{x} \<br>        b_{y} \<br>        b_{h} \<br>        b_{w} \<br>        c_{1} \<br>        c_{2} \<br>        c_{3}<br>        \end{bmatrix} \<br>      $$<br>      3）对象分配  </p>
<pre><code>  * 将对象分配给包含对象中点的格子，**即使对象可以横跨多个格子**；

*   即使格子有两辆车的一部分，仍认为中心格子没有任何我们感兴趣对对象；  
  4）边框约定：  </code></pre><p>$$<br>$$<br>        $b_{x},b_{y},b_{h},b_{w}$ 单位是相对于格子尺寸的比例，所以 $b_{x},b_{y}$ 必须在0和1之间, $b_{h},b_{w}$ 可能会大于1.<br>        - ### 交并比（Intersection over union）</p>
<pre><code>用来评价对象检测算法。  </code></pre><p>$$<br>          IOU =  \frac{(A \cap B)}{(A \cup B)}  \\<br>          A : 实际边界框 \<br>          B : 算法预测边界框<br>          $$<br>          一般约定：如果$ IoU ≥ 0.5 $，就说检测正确，如果预测和实际边界框完美重叠，IoU就是1.  </p>
<h3 id="非极大值抑制（Non-max-suppression）"><a href="#非极大值抑制（Non-max-suppression）" class="headerlink" title="非极大值抑制（Non-max suppression）"></a>非极大值抑制（Non-max suppression）</h3><pre><code>这是一个基于交并比的工具，可以让YOLO算法输出效果更好。  
1）问题提出：  
前面的算法可能对同一个对象做出多次检测，即：多个格子都认为包含对象的中心点，这造成对象不是被检测出一次而是检测出多次。  
**非极大值抑制** 可以确保每个对象只被检测出一次。  
2）步骤：  </code></pre><p>$$<br>$$<br>            * 找 $ P_{ c}$最大的边界框 - 最可靠的检测；<br>            * 计算剩余边界框与上述概率最大边界框（不是实际边界框）的交并比，高度重叠点边界框输出被抑制（表示预测的为同一个对象）；<br>            * 处理下一个概率最高的边界框；  </p>
<p>总结：输出概率最大的分类结果，但抑制很接近但不是概率最大的其他预测结果。</p>
<h3 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h3><pre><code>解决的问题：当一个格子中同时落入多个对象的中心点，让一个格子检测出多个对象。  
Anchor box思路：预先定义多个不同形状的 anchor box，把预测结果和 anchor box 关联起来。  

  * **标签定义**  </code></pre><p>重复两次（或多次）的向量元素：<br>$ y = [P_{ c} b_{ x} b_{ y} b_{ h} b_{ w} c_{ 1} c_{ 2} c_{ 3} P_{ c} b_{ x} b_{ y} b_{ h} b_{ w} c_{ 1} c_{ 2} c_{ 3}]^{ T} $<br>即：输出维度更高，与anchor box 数量相关。  </p>
<p>问题：<br>            * 引入 anchor box 是为处理两个对象出现在同一个格子里的情况；<br>            * 对一个格子中有三个对象的情况处理不好；<br>            * 两个对象 anchor box 形状也一样时，处理也不好；  </p>
<p>手工指定 anchor box 形状；</p>
<h2 id="4、Yolo-算法"><a href="#4、Yolo-算法" class="headerlink" title="4、Yolo 算法"></a>4、Yolo 算法</h2><pre><code>组合前面课程的组件。  
  - ### 训练集构造

      * 标签定义  </code></pre><p>$ 网格 × 网格 × anchor box × 目标向量（ P_{ c},b_{ x},b_{ y},b_{ h},b_{ w}, c_{ 1}, c_{ 2}, c_{ 3} )$</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><h3 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h3><h2 id="样例：车辆检测代码"><a href="#样例：车辆检测代码" class="headerlink" title="样例：车辆检测代码"></a>样例：车辆检测代码</h2><p>  输入：（m，608，608，3）<br>  Yolo输出：19 * 19, 5, 85（由cnn模型输出）<br>  处理流程：<br>  1、通过分类分数过滤<br>  将 19 * 19 * 5 * 85 分解成以下tensor<br>    * box_confidence: 19 * 19 , 5, 1<br>    * boxes: 19 * 19, 5, 4<br>    * box_class_prob: 19 * 19, 5, 80<br>  1）对19*19个cell中的每个cell操作其5个anchor box，取每个box所识别出的目标分类及分数：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">box_classes = K.argmax(box_scores, axis=-<span class="number">1</span>)</span><br><span class="line">box_classes_scores = K.<span class="built_in">max</span>(box_scores, axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>  2）依据box分类分数过滤，得到分类掩码：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filtering_mask = box_class_scores &gt;= threshold</span><br></pre></td></tr></table></figure><br>  3）过滤<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores = tf.boolean_mask(box_class_scores, filtering_mask)</span><br><span class="line">boxes = tf.boolean_mask(boxes, filtering_mask)</span><br><span class="line">classes = tf.boolean_mask(box_classes, filtering_mask)</span><br></pre></td></tr></table></figure><br>  <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a><br>  处理结果为：对每个cell保留分类值大于域值的box。  </p>
<p>  2、非极大值抑制<br>  1）计算IOU<br>  2）非极大值抑制处理：<br>    * 选择分数最高的box<br>    * 计算所选box与所有其他box的iou，移除超过iou_threshold的box；<br>    * 循环处理剩余box中分数最高对box，进行上述处理；<br>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.image.non_max_suppression()  <span class="comment"># 返回抑制操作后的索引清单</span></span><br><span class="line">K.gather()                      <span class="comment"># 返回索引清单中对应的对象</span></span><br></pre></td></tr></table></figure></p>
<p>  3、box坐标转换<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">boxes = yolo_boxes_to_corners(box_xy, box_wh) <span class="comment"># 转换为对角坐标</span></span><br><span class="line">boxes = scale_boxes(boxes, image_shape) <span class="comment"># 缩放</span></span><br></pre></td></tr></table></figure></p>
<p>  4、yolo.h5 预训练权重下载<br>  load_model() 报错处理<br>  <a target="_blank" rel="noopener" href="https://blog.cndn.net/Solo95/article/details/85262828">https://blog.cndn.net/Solo95/article/details/85262828</a>  </p>
<p>  ** 总结：**<br>  1）样本通过yolo_model生成yolo_model.output;<br>  2）yolo_model.output 通过 yolo_head 转换格式为：yolo_outputs;<br>  3）yolo_outputs通过 yolo_eval 预测生成scores,boxes,classes;  </p>
<h1 id="四、人脸识别"><a href="#四、人脸识别" class="headerlink" title="四、人脸识别"></a>四、人脸识别</h1><h2 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h2><pre><code>* 人脸验证(face verification):输入为图片以及ID等信息以验证是否这个人，1对1的问题。
* 人脸识别(face recognition):1对多的问题，相当于多次的人脸验证。维护一个K个样本的数据库，输入为一个图片。</code></pre><h2 id="2、one-shot-learning"><a href="#2、one-shot-learning" class="headerlink" title="2、one-shot learning"></a>2、one-shot learning</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><pre><code>人脸验证的问题，通常只有单个或极少的样本进行训练以识别出一个人。  </code></pre><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="1）常规方案：softmax分类"><a href="#1）常规方案：softmax分类" class="headerlink" title="1）常规方案：softmax分类"></a>1）常规方案：softmax分类</h4><pre><code>将人的照片放进卷积神经网络，不同的人对应不同的标签。  
存在缺点：  
  * 训练集不足以训练稳健的神经网络
  * 新人加入需要增加新的标签分类</code></pre><h4 id="2）Similarity函数"><a href="#2）Similarity函数" class="headerlink" title="2）Similarity函数"></a>2）Similarity函数</h4><p>$$<br>$$</p>
<pre><code>训练一个函数：d(image1, image2)=degree of different images。  
以两张图片作为输入,然后输出这两张图片的差异值。 如果你放进同一个人的两张照片, 你希望它能输出一个很小的值, 如果放进两个长相差别很大的人的照片, 它就输出一个很大的值。阈值 $\to$ 是一个超参数。  </code></pre><h2 id="3、Siamese（孪生）-网络"><a href="#3、Siamese（孪生）-网络" class="headerlink" title="3、Siamese（孪生） 网络"></a>3、Siamese（孪生） 网络</h2><pre><code>定义：  
  对于两个不同的输入，运行相同的卷积神经网络，然后比较它们，这种架构称为siamese网络架构。  
流程：  </code></pre><p>$$<br>$$<br>      1）输入两张图片 $（x^{(1)},x^{(2)}）$ ；<br>      2）对输入图片分别编码(通过卷积神经网络，得到softmax分类前的向量)，得到 $(f(x^{(1)}), f(x^{(2)}))$ ;<br>      3）定义两幅图片编码之差的范数：<br>      4）反向传播改变这个网络所有层的参数，得到不同的编码结果，以满足相似度条件（即：得到好的编码）；  </p>
<h3 id="参数训练方案：Triplet损失"><a href="#参数训练方案：Triplet损失" class="headerlink" title="参数训练方案：Triplet损失"></a>参数训练方案：Triplet损失</h3><pre><code>  通过学习神经网络的参数来得到优质的**人脸图片编码**, 方法之一就是定义三元组损失函数然后应用梯度下降。  

**定义：**  
  1）三元组输入  
    * Anchor图片样本（A）
    * Positive图片样本（P）
    * Negative图片样本（N）  
  2）损失函数公式  </code></pre><p>$$<br>          d(A, P) = \mid\mid f(A)-f(P) \mid\mid^{2} \<br>          d(A, N) = \mid\mid f(A)-f(N) \mid\mid^{2} \<br>          d(A, P) + \alpha \le d(A, N) \<br>          $$           $\alpha$ : 称为间隔的超参数，用以防止出现函数f返回0，同时又满足上述条件的无用结果。<br>          $\alpha$ 至少为0.2，它拉大Anchor和Positive图片对与Anchor和Negative图片对之间的差距。<br>          <strong>损失函数：</strong><br>$$<br>          L(A,P,N) = max(\mid\mid f(A)-f(P) \mid\mid^{2} - \mid\mid f(A)-f(N) \mid\mid^{2} + \alpha, 0)<br>          $$<br>          3）三元组训练集<br>            * 需要成对的A和P，这要求确保有同一个人的多个图片。<br>            * 尽可能选择 <strong>难训练</strong> 的三元组A、P和N，即：正负样本与Anchor样本方差值相差不大。  </p>
<p><strong>参考资料：</strong> Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). FaceNet: A Unified Embedding for Face Recognition and Clustering</p>
<h3 id="参数训练方案：人脸验证与二分类"><a href="#参数训练方案：人脸验证与二分类" class="headerlink" title="参数训练方案：人脸验证与二分类"></a>参数训练方案：人脸验证与二分类</h3><pre><code>使用 Siamese 网络，计算出一对图像对编码，然后将其输入到逻辑回归单元以进行预测，如果是相同的人输出是1，不同的人输出为0。这种训练方法可以作为 **Triplet loss** 方法对替代。  

1）**预测值计算公式：**  </code></pre><p>$$<br>          \hat{y} = \sigma{(\sum_{k=1}^{128}w_{k}\mid f(x^{(i)})_{k} -f(x^{(j)})_{k} \mid + b)}<br>$$           和普通的逻辑回归一样，在128个单元上训练合适对权重，用来预测两张图片是否是一个人。  </p>
<pre><code>**公式变种：** $\chi平方相似公式$  </code></pre><p>$$<br>          \frac{(f(x^{(i)})_{k}-f(x^{(j)})_{k})^{2}}{f(x^{(i)})_k + f(x^{(j})_{k}}<br>$$<br>          <strong>参考资料：</strong> Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). DeepFace: Closing the gap to human-level performance in face verification  </p>
<pre><code>2）训练集  
创建一个只有成对图片的训练集，不是三个一组而是成对的图片，目标标签是1，表示一对图片是一个人，目标标签为0则为不同人。  </code></pre><h1 id="五、神经风格迁移"><a href="#五、神经风格迁移" class="headerlink" title="五、神经风格迁移"></a>五、神经风格迁移</h1><p>  定义：使用 <strong>风格图片（S）</strong> 的风格，重新根据 <strong>内容图片（C）</strong> 的内容生成 <strong>新的图片（G）</strong>。  </p>
<h2 id="1、卷积网络提取的特征"><a href="#1、卷积网络提取的特征" class="headerlink" title="1、卷积网络提取的特征"></a>1、卷积网络提取的特征</h2><pre><code>接受域  </code></pre><h2 id="2、神经风格迁移代价函数定义"><a href="#2、神经风格迁移代价函数定义" class="headerlink" title="2、神经风格迁移代价函数定义"></a>2、神经风格迁移代价函数定义</h2><pre><code>代价函数的目的：此前训练神经网络的代价函数定义的是关于参数的函数，此处的代价函数则是关于生成图片G的函数。通过梯度下降去最小化J(G)，以生成符合要求的图像。  

代价函数由两部分组成：  </code></pre><p>$$<br>      J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G)<br>$$        </p>
<ul>
<li>内容代价： $\alpha J_{content}(C, G)$<pre><code>* 风格代价： $\beta J_&#123;style&#125;(S, G)$  </code></pre></li>
</ul>
<p>迭代更新：$ G = G - frac{∂}{∂ G}J(G)$</p>
<h3 id="内容代价函数"><a href="#内容代价函数" class="headerlink" title="内容代价函数"></a>内容代价函数</h3><pre><code>**“内容”定义**  
用隐含层l的激活项来计算内容代价，如果l很小（如：1），生成图片像素上就会非常接近内容图片。  

**内容代价函数定义：**  </code></pre><p>$$<br>          J_{content}(C, G)=\frac{1}{2}\mid\mid a^{[l](C)} - a^{[l](G)} \mid\mid ^{2}<br>$$    </p>
<h3 id="风格代价函数"><a href="#风格代价函数" class="headerlink" title="风格代价函数"></a>风格代价函数</h3><pre><code>**&quot;风格&quot;定义**  
图片同一坐标位置（ $N_&#123;H&#125;, N_&#123;W&#125;$ ）值，在不同通道（ $N_&#123;C&#125;$ ）之间的相互关系。  
使用 **风格矩阵** 进行表示：  </code></pre><p>$$<br>          G_{kk’}^{[l](S)} = \sum_{i=1}^{n_{H}^{[l]}}\sum_{j=1}^{n_{W}^{[l]}} a_{i,j,k}^{[l](S)}a_{i,j,k’}^{[l](S)} \<br>          G_{kk’}^{[l](G)} = \sum_{i=1}^{n_{H}^{[l]}}\sum_{j=1}^{n_{W}^{[l]}} a_{i,j,k}^{[l](G)}a_{i,j,k’}^{[l](G)}<br>$$            </p>
<ul>
<li>如果两个通道中的激活项数值很大，那么 $G_{kk’}^{[l]}$ 会很大，如不相关则计算值很小。  </li>
</ul>
<p><strong>风格代价函数定义</strong><br>$$<br>J_{style}^{[L]}(S, G)=\frac{1}{(2n_{H}^{[l]}n_{W}^{[l]}n_{C}^{[l]})^{2}}\sum_{k}\sum_{k’} (G_{kk’}^{[l](S)} - G_{kk’}^{[l](G)}) \<br>\<br>J_{style}(S, G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)<br>$$<br>            * 这是 <strong>一个隐层</strong> 两个矩阵间一个基本的 Frobenius 范数,也就是?图像和?图像之间的范数再乘上一个归一化常数。<br>            * 为取得更好的结果，可以对各个隐层都计算风格代价函数，对每个层定义权重-超参数： $\lambda^{[l]}$ 。</p>
<h1 id="六、一维到三维推广应用"><a href="#六、一维到三维推广应用" class="headerlink" title="六、一维到三维推广应用"></a>六、一维到三维推广应用</h1><h2 id="一维应用"><a href="#一维应用" class="headerlink" title="一维应用"></a>一维应用</h2><pre><code>* 一维信号数据：脑电图</code></pre><h2 id="三维应用"><a href="#三维应用" class="headerlink" title="三维应用"></a>三维应用</h2><pre><code>* CT扫描
* 视频数据</code></pre><h1 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h1><pre><code>* 基于 Leon Gatys,  Alexandra Ecker 和 Matthias Bethge 的这篇论文。 Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015).  </code></pre><p>A Neural Algorithm of Artistic Style (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>)<br>    * Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). FaceNet: A Unified Embedding for Face Recognition and Clustering<br>    * Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). DeepFace: Closing the gap to human-level performance in face verification<br>    * The pretrained model we use is inspired by Victor Sy Wang’s implementation and was loaded using his code: <a target="_blank" rel="noopener" href="https://github.com/iwantooxxoox/Keras-OpenFace">https://github.com/iwantooxxoox/Keras-OpenFace</a>.<br>    * Our implementation also took a lot of inspiration from the official FaceNet github repository: <a target="_blank" rel="noopener" href="https://github.com/davidsandberg/facenet">https://github.com/davidsandberg/facenet</a><br>    * Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>)<br>    * Harish Narayanan, Convolutional neural networks for artistic style transfer.<br><a target="_blank" rel="noopener" href="https://harishnarayanan.org/writing/artistic-style-transfer/">https://harishnarayanan.org/writing/artistic-style-transfer/</a><br>    * Log0, TensorFlow Implementation of “A Neural Algorithm of Artistic Style”.<br><a target="_blank" rel="noopener" href="http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style">http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style</a><br>    * Karen Simonyan and Andrew Zisserman (2015). Very deep convolutional networks for largescale image recognition (<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a>)<br>    * MatConvNet. <a target="_blank" rel="noopener" href="http://www.vlfeat.org/matconvnet/pretrained/">http://www.vlfeat.org/matconvnet/pretrained/</a></p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/08/15/fifth-git-blog/" data-id="cllqdwhcy000010h7fy1oa07g" class="article-share-link" data-share="baidu" data-title="第四章：卷积神经网络">Share</a>
      

      
        <a href="http://example.com/2023/08/15/fifth-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-four-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/08/05/four-git-blog/" class="article-date">
  <time datetime="2023-08-05T08:05:58.000Z" itemprop="datePublished">2023-08-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/four-git-blog/">第三章：结构化机器学习项目</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>作者先以一个启发性的例子，来阐述如何改善系统？根据第二门课的内容，我们可以收集更多的训练数据，，使用不同的优化算法 ，例如 Adam 优化算法，或者尝试使用规模更大或者更小的神经网络，例如提到的 Dropout或者L2正则化。<br>吴恩达提到如果做了错误的选择，可能会浪费大量的时间和人力，所以在第三门课，会介绍一些策略或者分析机器学习的方法，让大家朝着最有希望的方向前进。  </p>
</li>
<li></li>
<li><h1 id="一、机器学习策略-目标定义"><a href="#一、机器学习策略-目标定义" class="headerlink" title="一、机器学习策略 - 目标定义"></a>一、机器学习策略 - 目标定义</h1><p>解决如何更快速、更高效构建机器学习系统的方法。  </p>
</li>
<li><h2 id="1、正交化-Orthogonalization-评估对策"><a href="#1、正交化-Orthogonalization-评估对策" class="headerlink" title="1、正交化(Orthogonalization) - 评估对策"></a>1、正交化(Orthogonalization) - 评估对策</h2><p>在机器学习中表示下述四个方面的指标具有独立性：  </p>
<ul>
<li>系统在训练集上表现不错；</li>
<li>系统在开发集上有好的表现；</li>
<li>系统在测试集上有好的表现；</li>
<li>测试集上系统的成本函数在真实数据中有好的表现；<table>
<thead>
<tr>
<th>数据集</th>
<th>指标描述</th>
<th>调优方式（独立按钮）</th>
</tr>
</thead>
<tbody><tr>
<td>训练集</td>
<td>系统不能很好地拟合训练集</td>
<td>训练更大网络；切换到更好的优化算法；</td>
</tr>
<tr>
<td>开发集</td>
<td>系统不能更好地拟合开发集</td>
<td>正则化；增大训练集；</td>
</tr>
<tr>
<td>测试集</td>
<td>开发集拟合很好，测试集拟合不好，即开发集过拟合</td>
<td>更大的开发集</td>
</tr>
<tr>
<td>真实数据</td>
<td>开发集分布设置不正确,要么你的成本函数测量的指标不对。</td>
<td>改变开发集（同分布）；改变成本函数；</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><h2 id="2、评估指标"><a href="#2、评估指标" class="headerlink" title="2、评估指标"></a>2、评估指标</h2><p>基于评估指标确定何种改进方式。  </p>
</li>
<li><h3 id="1）单一数字评估指标-Single-number-evaluation-metric"><a href="#1）单一数字评估指标-Single-number-evaluation-metric" class="headerlink" title="1）单一数字评估指标(Single number evaluation metric)"></a>1）单一数字评估指标(Single number evaluation metric)</h3><p>评估分类器的指标：  </p>
<ul>
<li>查准率（precision）：样例定义为在分类器标记为猫的样例中真实为猫的比例；</li>
<li>查全率（recall）：在样本集中所有为猫的样本中，能被分类器识别出的比例；</li>
<li>F1分数（单实数评估指标）：为查准率和查全率的平均值，用在开发集上计算以进行综合评价，公式为：<br>$$<br>F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}} \<br>P: 查准率 \<br>R: 查全率<br>$$</li>
</ul>
</li>
<li><h3 id="2）-满足指标、优化指标"><a href="#2）-满足指标、优化指标" class="headerlink" title="2） 满足指标、优化指标"></a>2） 满足指标、优化指标</h3><p>当设定单实数评估指标比较困难时，可设定满足、优化指标。  </p>
<ul>
<li>满足指标：必须达到</li>
<li>优化指标：可以持续改进<br>如果你要考虑N个指标,有时候选择其中 一个 指标做为优化指标是合理的。所以你想尽量优化那个指标,然后剩下N−1个指标都是满足指标,意味着只要它们达到一定阈值, 例如运行时间快于 100 毫秒, 但只要达到一定的阈值,你不在乎它超过那个门槛之后的表现,但它们必须达到这个门槛。  </li>
</ul>
</li>
</ul>
<p>上述指标必须在训练集或者开发集或者测试集上计算而来。</p>
<ul>
<li><h2 id="3、设立数据集"><a href="#3、设立数据集" class="headerlink" title="3、设立数据集"></a>3、设立数据集</h2><ul>
<li>训练集：训练不同的模型；</li>
<li>开发集：使用单实数评估指标评估不同的思路，然后选择一个，继续迭代以改善开发集性能直到得到一个低的损失函数值；</li>
<li>测试集：评估；<br>将所有数据随机洗牌，放入开发集和测试集以使数据同分布。开发集和测试集必须为同分布的数据。  </li>
</ul>
</li>
</ul>
<p>设置开发集和评估指标的意义在于定义了机器学习项目的目标而训练集的构建方式决定了达成目标的快慢。</p>
<ul>
<li><h3 id="开发集和测试集大小"><a href="#开发集和测试集大小" class="headerlink" title="开发集和测试集大小"></a>开发集和测试集大小</h3><table>
<thead>
<tr>
<th>数据集特征</th>
<th>训练集比例</th>
<th>开发集比例</th>
<th>测试集比例</th>
</tr>
</thead>
<tbody><tr>
<td>一万个以下的样本</td>
<td>60</td>
<td>20</td>
<td>20</td>
</tr>
<tr>
<td>规模比较大的样本</td>
<td>98</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>规则：<br>把大量的数据分到训练集，然后少量数据分到开发集和测试集。  </p>
</li>
<li><h2 id="4、改变开发-测试集和改变指标"><a href="#4、改变开发-测试集和改变指标" class="headerlink" title="4、改变开发/测试集和改变指标"></a>4、改变开发/测试集和改变指标</h2><p>开发集和评估指标的确定就相当于确定了项目目标，改变则意味着项目目标的调整。而当项目有新的考虑因素加入时则确实需要调整项目目标。  </p>
</li>
<li><h3 id="1）修改评估指标"><a href="#1）修改评估指标" class="headerlink" title="1）修改评估指标"></a>1）修改评估指标</h3><p>例：初始分类错误指标  </p>
</li>
</ul>
<p>$$<br>Error = \frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}}I\{y^{(i)}_{pred}\ne y^{(i)}\}<br>$$</p>
<p>  $m_{dev}$ 为开发集样本数<br>  $y^{(i)}_{pred}$ 表示分类预测值，为0或1<br>  $I$ 为一个函数，返回表达式是否为真，真为1，假为0  </p>
<p>  增加权重项的分类错误评估指标：  </p>
<p>$$<br>Error = \frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}}w^{(i)}I\{y^{(i)}_{pred}\ne y^{(i)}\}<br>$$<br>  $w^{(i)}$ ` 对于特定类型样本，可以增加10～100倍的惩罚权重，从而使其有较大到错误指标值。  </p>
<p>  归一化处理：  </p>
<p>$$<br>  Error = \frac{1}{\sum{m_{dev}}}\sum_{i=1}^{m_{dev}}w^{(i)}I\{y^{(i)}_{pred}\ne y^{(i)}\}<br>  $$<br>  <strong>结论：</strong><br>  如果评估指标无法给出按你到需求给出的好算法排名，则需要调整评估指标。  </p>
<ul>
<li><h3 id="2）修改开发测试集"><a href="#2）修改开发测试集" class="headerlink" title="2）修改开发测试集"></a>2）修改开发测试集</h3><p>当实际数据与所使用开发测试集数据在质量上存在差异，造成实际使用效果不好，则需要修改评估指标或者调整开发测试集。  </p>
</li>
<li><h2 id="5、误差分析"><a href="#5、误差分析" class="headerlink" title="5、误差分析"></a>5、误差分析</h2></li>
<li><h3 id="1）贝叶斯最优错误率（Bayesian：Bayes-optimal-error）"><a href="#1）贝叶斯最优错误率（Bayesian：Bayes-optimal-error）" class="headerlink" title="1）贝叶斯最优错误率（Bayesian：Bayes optimal error）"></a>1）贝叶斯最优错误率（Bayesian：Bayes optimal error）</h3><p>理论上可能达到到最优错误率。  </p>
<ul>
<li>人类表现在很多任务中离贝叶斯错误率已经不远；</li>
<li>只要算法表现比人类表现差，就可以使用工具提高性能；  </li>
</ul>
</li>
</ul>
<p>以人类错误率近似贝叶斯错误率，算法错误率与贝叶斯错误率之间到差值为算法偏差，可以通过前述：加大训练集，改进算法，偏差和方差分析进行优化。</p>
<ul>
<li><h3 id="2）可避免偏差（Avoidable-bias"><a href="#2）可避免偏差（Avoidable-bias" class="headerlink" title="2）可避免偏差（Avoidable bias)"></a>2）可避免偏差（Avoidable bias)</h3><p>贝叶斯错误率或者贝叶斯错误率估计值（人类错误率）与训练错误率之间的差值称为<strong>可避免偏差</strong>，训练错误率的绝对值即为<strong>偏差</strong>。  </p>
</li>
<li><h3 id="3）超过人的表现时的进一步优化"><a href="#3）超过人的表现时的进一步优化" class="headerlink" title="3）超过人的表现时的进一步优化"></a>3）超过人的表现时的进一步优化</h3><p>使用现有的一些方法就难以给出优化方向的指引了。  </p>
</li>
<li><h2 id="6、改善模型的表现"><a href="#6、改善模型的表现" class="headerlink" title="6、改善模型的表现"></a>6、改善模型的表现</h2><table>
<thead>
<tr>
<th>误差分析结果</th>
<th>应对方式</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>减少可避免偏差</td>
<td>更大的模型；训练更久；优化算法；更好的神经网络架构；更好的超参数</td>
<td></td>
</tr>
<tr>
<td>减少方差</td>
<td>收集更多的数据（看到原有开发集缺乏的数据）；正则化；不同的神经网络架构；更好的超参数</td>
<td></td>
</tr>
</tbody></table>
</li>
<li><h1 id="二、机器学习策略"><a href="#二、机器学习策略" class="headerlink" title="二、机器学习策略"></a>二、机器学习策略</h1></li>
<li><h2 id="1、错误分析"><a href="#1、错误分析" class="headerlink" title="1、错误分析"></a>1、错误分析</h2><p>分析产生错误的真正原因，以确定是否值得去处理该类误差。<br>使用表格对错误样本将错误分类，给出各种错误类型的占比。  </p>
<p>错误分析有别于之前的偏差、方差分析，错误分析集中于对错误样本之所以出错的原因分析，从而决策是否做出相应的应对处理。  </p>
<ul>
<li>假阳性/假阴性错误</li>
<li>标记错误<br>深度学习对于训练集中的<strong>随机错误</strong>是相当健壮的。只要错误是随机的就可以不管。而对于<strong>系统性错误</strong>则没那么健壮。<br>对于开发集和测试集上的标记错误，可以作为单独一类错误分类，视严重程度决定是否进行处理。</li>
</ul>
</li>
<li><h2 id="2、快速搭建系统"><a href="#2、快速搭建系统" class="headerlink" title="2、快速搭建系统"></a>2、快速搭建系统</h2><p>路线图：  </p>
<ul>
<li>定义目标：设立开发集、测试集和指标</li>
<li>搭建系统模型</li>
<li>找到训练集进行训练</li>
<li>在开发集、测试集上进行评估</li>
<li>偏差方差分析</li>
<li>错误分析</li>
</ul>
</li>
<li><h2 id="3、训练数据-使用与开发、测试集不同分布的数据"><a href="#3、训练数据-使用与开发、测试集不同分布的数据" class="headerlink" title="3、训练数据 - 使用与开发、测试集不同分布的数据"></a>3、训练数据 - 使用与开发、测试集不同分布的数据</h2></li>
<li><h3 id="1）方法一：shuffle-数据"><a href="#1）方法一：shuffle-数据" class="headerlink" title="1）方法一：shuffle 数据"></a>1）方法一：shuffle 数据</h3><ul>
<li>优点：实现了训练数据与开发、测试集数据的同分布；</li>
<li>缺点：混杂后开发、测试集数据中原有的数据分布特性被极大稀释，造成前述系统目标的偏离，从而是针对不同于实际的数据去优化。</li>
</ul>
</li>
<li><h3 id="2）方法二：不做shuffle"><a href="#2）方法二：不做shuffle" class="headerlink" title="2）方法二：不做shuffle"></a>2）方法二：不做shuffle</h3><ul>
<li>优点：开发集、测试集数据完全来自实际的数据分布，系统优化目标就是实际要处理的目标；</li>
<li>缺点：训练集与开发、测试集数据不同分布；</li>
</ul>
</li>
<li><h4 id="数据分布不匹配的偏差、方差分析"><a href="#数据分布不匹配的偏差、方差分析" class="headerlink" title="数据分布不匹配的偏差、方差分析"></a>数据分布不匹配的偏差、方差分析</h4><p>定义新的数据子集：<strong>训练-开发集</strong>：随机打散<strong>训练集</strong>，然后分出一部分训练集作为训练-开发集（不在训练-开发集上跑后向传播）。<br>误差分析：  </p>
<ul>
<li>训练集误差</li>
<li>训练-开发集误差</li>
<li>开发集误差<br>通过训练集误差和训练-开发集误差判断是否方差过大，存在<strong>泛化问题</strong>。<br>当训练-开发集误差与开发集误差相差过大，则存在<strong>数据不匹配问题</strong>。  </li>
</ul>
</li>
</ul>
<p>错误率分析表格<br>      | 错误率分类 | 非同分布数据集 | 同分布数据集（真实数据） |<br>      |—|—|—|<br>      | 人类平均水平 | | 真实数据人类错误率 |<br>      | 训练集错误率 | | 真实数据训练错误率 |<br>      | 未训练数据错误率 | 训练-开发集错误 | 开发集 / 测试集错误 |</p>
<p>通过错误率分析表格进行差错分析，可分析出几类问题：<br>    * 可避免偏差<br>    * 方差<br>    * 数据不匹配</p>
<ul>
<li><h4 id="数据不匹配问题处理"><a href="#数据不匹配问题处理" class="headerlink" title="数据不匹配问题处理"></a>数据不匹配问题处理</h4></li>
<li><h2 id="4、迁移学习"><a href="#4、迁移学习" class="headerlink" title="4、迁移学习"></a>4、迁移学习</h2><p>定义：有的时候神经网络可以从一个任务中学习得到的知识，并将这些知识应用到一个独立任务中。<br>迁移学习过程：  </p>
<ul>
<li>1）在第一阶段学习过程中，当进行图像识别任务训练时，可以训练神经网络的所有常用参数，所有的权重，所有的层 - 得到一个图像识别预测的网络；</li>
<li>2）把数据集换成（x,y）对， 迁移学习要做的是初始化最后一层的权重 - $w^{[l]}和b^{[l]}$ 随机初始化；</li>
<li>3）重新训练最后一层的权重 $w^{[l]}和b^{[l]}$，并保持其他参数不变；  </li>
</ul>
</li>
</ul>
<p>经验规则：<br>    * 小数据集：只训练输出层前的最后一层，或者是最后一两层；<br>    * 很多数据：重新训练网络中的所有参数；两个阶段：预训练（pre-training）,微调（fine tuning）；  </p>
<p>迁移学习应用场合：<br>迁移来源问题中有很多数据，但迁移问题没有那么多数据。任务A的低层次特征,可以帮助任务B的学习,那迁移学习更有意义一些。</p>
<ul>
<li><h2 id="5、多任务学习"><a href="#5、多任务学习" class="headerlink" title="5、多任务学习"></a>5、多任务学习</h2><p>定义：让单个神经网络同时做几件事情，然后希望每个任务都能帮到其他任务。<br>多任务学习的标签不是 1 x 1 的，而是 n x 1 的。  </p>
<p>损失函数定义：  </p>
</li>
</ul>
<p>$$<br>L = \sum{_{i=1}^{m}}\sum{_{j=1}^{n}}L(\hat{y}^{(i)}_{j}, y_{j}^{(i)}) \<br>$$</p>
<p>$$<br>L(\hat{y}^{(i)}_{j}, y^{(i)}_{j}) = -y^{(i)}_{j}\log{\hat{y}^{(i)}_{j}} - (1-y^{(i)}_{j})\log(1-{\hat{y}^{(i)}_{j}})<br>$$</p>
<p>  如果可以训练一个足够大的神经网络, 那么多任务学习肯定不会或者很少会降低性能, 我们都希望它可以提升性能, 比单独训练神经网络来单独完成各个任务性能要更好。  </p>
<ul>
<li><h2 id="5、端到端学习"><a href="#5、端到端学习" class="headerlink" title="5、端到端学习"></a>5、端到端学习</h2><p>定义：一些数据处理系统或者学习系统,它们需要多个阶段的处理。端到端深度学习就是忽略所有这些不同的阶段,用单个神经网络代替它。端到端深度学习的挑战之一是,你可能需要大量数据才能让系统表现良好。  </p>
<p>任务分解带来的好处：<br>1）分解后的任务要简单很多；<br>2）子任务的训练数据都很多；<br>如：人脸识别系统将任务分解为：人脸定位；人脸识别两个子任务。  </p>
</li>
<li><h3 id="端到端学习的优点"><a href="#端到端学习的优点" class="headerlink" title="端到端学习的优点"></a>端到端学习的优点</h3><ul>
<li>端到端学习真的只是让数据说话。 所以如果你有足够多的(x,y)数据,那么不管从x到y最适合的函数映射是什么,如果你训练一个足够大的神经网络, 希望这个神经网络能自己搞清楚, 而使用纯机器学习方法, 直接从x到y输入去训练的神经网络,可能更能够捕获数据中的任何统计信息,而不是被迫引入<strong>人类的成见</strong>。</li>
<li>所需手工设计的组件更少, 所以这也许能够简化你的设计工作流程,你不需要花太多时间去手工设计功能,手工设计这些中间表示方式。如：语音识别中音位的概念。</li>
</ul>
</li>
<li><h3 id="端到端学习的缺点"><a href="#端到端学习的缺点" class="headerlink" title="端到端学习的缺点"></a>端到端学习的缺点</h3><ul>
<li>需要大量的数据</li>
<li>排除了可能有用的手工设计组件。 机器学习研究人员一般都很鄙视手工设计的东西, 但如果你没有很多数据, 你的学习算法就没办法从很小的训练集数据中获得洞察力。</li>
</ul>
</li>
<li><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>学习算法的两个主要知识来源：数据、手工设计的任何东西。当你有大量数据时,手工设计的东西就不太重要了, 但是当你没有太多的数据时, 构造一个精心设计的系统, 实际上可以将人类对这个问题的很多认识直接注入到问题里,进入算法里应该挺有帮助的。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/08/05/four-git-blog/" data-id="cllqdwhd4000410h7h1pxcea5" class="article-share-link" data-share="baidu" data-title="第三章：结构化机器学习项目">Share</a>
      

      
        <a href="http://example.com/2023/08/05/four-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hexo-equation-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/27/hexo-equation-blog/" class="article-date">
  <time datetime="2023-07-27T08:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hexo/">Hexo</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/hexo-equation-blog/">Hexo如何解决公式渲染问题</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>  找到Hexo可以搭建博客框架，着实高兴了好几天，感觉以后可以把各种深度挖掘研究的笔记转移到这个战场了。但真正开始使用，发现了好多问题，其中最大的一个问题是公式渲染问题。网上找了各种资料，例如将现在hexo-renderer-marked换成hexo-renderer-kramed，这个我也有尝试，对于行间的公式渲染没有问题了，但是这个无法解决的是行内的公式，也没有找到特别好的办法。</p>
<h2 id="方法简介"><a href="#方法简介" class="headerlink" title="方法简介"></a>方法简介</h2><p>   可能因为文章确实存在很久没有更新的问题，所以得到的结果并不能有效解决当前所有的问题。<br>这个时间正好看到有篇文章提到，还是继续使用marked,只是换个版本的解决方案，这个解决了至少大多数的问题，具体的解决方式如下：</p>
<ul>
<li>打开hexo blog 目录：<br>cmd blog</li>
<li>卸载原先安装的marked<br>npm uninstall hexo-renderer-marked</li>
<li>重新安装marked 1.0.0版本<br>npm install <a href="mailto:hexo-rnderer-marked@1.0.0">hexo-rnderer-marked@1.0.0</a> </li>
<li>修改marked.js配置<br>编辑node_modules/marked/lib/marked.js<br>将第539行的<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([!&quot;#$%&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]\\^_`&#123;|&#125;~])/&#125; </span><br><span class="line">改为 </span><br><span class="line">escape: /^\\([!&quot;#$%&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]\\^_`&#123;|&#125;~])/&#125;)</span><br><span class="line">将第564行的 </span><br><span class="line">inline._escapes = /\\([!&quot;#$%&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]\\^_`&#123;|&#125;~])/g;</span><br><span class="line">改为:</span><br><span class="line">inline._escapes = /\\([!&quot;#$&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]^_`|~])/g;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>以上操作应该可以解决大多数公式渲染问题。</p>
<p>当然我也遇到过几个比较特殊的问题，也给大家分享下：<br>例如某个公式里面要用到<em>，可能由于一些原因，不能正确渲染，这时候建议在前面加上\，变为\</em><br>另外对于\frac公式，可以通过后面加上()等形式来解决。</p>
<p>针对特殊的符号渲染，目前的解决方案，只能像是调试程序一样，先一点点去定位问题，然后再去找到是什么问题造成的。对于不能渲染的符号，一般都可通过加\来达到目的，但是首先要知道是对于什么符号不能有效识别。</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/27/hexo-equation-blog/" data-id="cllqdwhd9000c10h72dnkd702" class="article-share-link" data-share="baidu" data-title="Hexo如何解决公式渲染问题">Share</a>
      

      
        <a href="http://example.com/2023/07/27/hexo-equation-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%95%AA%E5%A4%96%E7%AF%87/" rel="tag">番外篇</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-third-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/27/third-git-blog/" class="article-date">
  <time datetime="2023-07-27T08:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82/">参数调节</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/third-git-blog/">第二章 改善深层神经网络：超参数调试、正则化及优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="改善深层神经网络：超参数调试、正则化及优化"><a href="#改善深层神经网络：超参数调试、正则化及优化" class="headerlink" title="改善深层神经网络：超参数调试、正则化及优化"></a>改善深层神经网络：超参数调试、正则化及优化</h1><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>  通过第一门课的学习，我们对于神经网络的基本概念，逻辑回归和神经网络的类比，以及浅层和深层神经网络有了了解。这门课针对深层神经网络的超参数调试、正则化以及优化做进一步介绍，以便在实际应用的时候，对于超参数的选择，提供了很多方法和例子，加深我们的理解。有效的运作神经网络主要包括：超参数调优、如何构建数据、如何确保优化算法正常运行，从而使学习算法在合理时间内完成自我学习。  </p>
<h1 id="详情"><a href="#详情" class="headerlink" title="详情"></a>详情</h1><hr>
<h2 id="构建数据"><a href="#构建数据" class="headerlink" title="构建数据"></a>构建数据</h2><table>
    <tr>
        <td></td> <td>功能</td> <td>数据来源</td>  <td>小数据量</td> <td>大数据量</td>
    </tr>
    <tr>
        <td>训练集</td> <td>训练模型</td> <td></td>  <td>70%</td> <td>98%</td>
    </tr>
    <tr>
        <td>验证集</td> <td>选择模型</td> <td>保证与测试集同分布</td>  <td>&lt;10%~20%</td> <td>1%</td>
    </tr>
    <tr>
        <td>测试集</td> <td>评估模型</td> <td>保证与验证集同分布</td>  <td>&lt;10%~20%</td> <td>1%</td>
    </tr>
</table>

<h2 id="偏差误差"><a href="#偏差误差" class="headerlink" title="偏差误差"></a>偏差误差</h2><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><pre><code>* 初始模型训练完成后，评估算法的偏差高不高  </code></pre><p>偏差 高：评估训练集或训练数据的性能；</p>
<hr>
<p>如果无法拟合训练集，则需要<strong>选择一个新网络</strong>或者<strong>花更多时间训练网络</strong>或者<strong>尝试更先进的优化算法</strong>。<br>本阶段需达成的目标是：<strong>拟合训练集</strong>。<br>            * <strong>偏差</strong>降低到可接受程度后进行<strong>方差</strong>评估<br><strong>方差</strong>高：最好的办法是<strong>采用更多的数据</strong>；当无法获得更多数据的时候，采用<strong>正则化</strong>来减少过拟合。<br>            * 最终目标  ：<strong>低偏差、低方差</strong> 框架。</p>
<h3 id="偏差分析"><a href="#偏差分析" class="headerlink" title="偏差分析"></a>偏差分析</h3><pre><code>collapsed:: true
神经网络训练的流程显示模型训练的目标是**低偏差、低方差**，需依据偏差、方差的高低调整训练策略。  

  * **偏差：** 训练集误差与 **最优误差（贝叶斯误差）** 的比较结果；  </code></pre><p>最优误差：通常采用人的识别误差，如误差为0。<br>            * <strong>方差：</strong>验证集误差与 <strong>最优误差（贝叶斯误差）</strong> 的比较结果；</p>
<table>
<tr>
<td>误差分析</td> <td>特征</td> <td>处理</td>
</tr>
<tr>
<td>欠拟合</td> <td>偏差大：训练集误差大于最优误差  </td> <td><br>选择新网络<br><br>延长训练时间<br><br>尝试新的优化算法<br></td>
</tr>
<tr>
<td>过拟合</td> <td>方差大：验证集误差大于最优误差；<br>偏差小<br></td> <td><br>采用更多的数据训练<br><br>正则化<br></td>
</tr>
</table>

<h4 id="欠拟合问题处理"><a href="#欠拟合问题处理" class="headerlink" title="欠拟合问题处理"></a>欠拟合问题处理</h4><pre><code>消除欠拟合是模型训练的第一步目标，至少要拟合训练集，如无法达到则说明“方向”性错误。  </code></pre><h4 id="过拟合问题处理"><a href="#过拟合问题处理" class="headerlink" title="过拟合问题处理"></a>过拟合问题处理</h4><pre><code>* 更多的训练数据
* 正则化（当无法获取更多数据时）
* 数据生成</code></pre><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="（一）L2正则化"><a href="#（一）L2正则化" class="headerlink" title="（一）L2正则化"></a>（一）L2正则化</h3><h4 id="1、逻辑回归的正则化"><a href="#1、逻辑回归的正则化" class="headerlink" title="1、逻辑回归的正则化"></a>1、逻辑回归的正则化</h4><p>$$<br>              \text L2正则化：<br>              J(W, b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\mid\mid W \mid\mid_2^{2} \<br>              \text 向量参数 W 的 L2范数： \mid\mid W \mid\mid_2^{2} = W^{T}W = \sum_{j=1}^{n_{x}}w_j^{2} \<br>              W\in \mathbb{R}^{n_x}, b\in\mathbb{R} \<br>              \<br>              \text L1正则化：<br>              J(W, b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\mid\mid W \mid\mid_1 \<br>              \text 向量参数 W 的 L1范数： \mid\mid W \mid\mid_1 = \sum_{j=1}^{n_{x}}\mid w_j\mid \\<br>$$          </p>
<h4 id="2、神经网络的正则化"><a href="#2、神经网络的正则化" class="headerlink" title="2、神经网络的正则化"></a>2、神经网络的正则化</h4><p>$$<br>              \text L2正则化：<br>              J(W^{[1]}, b^{[1]},…,W^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\sum_{l=1}^L\mid\mid W^{[l]} \mid\mid_F^{2} \<br>              \text 矩阵范数（费罗贝尼乌斯范数）被定义为矩阵中所有元素的平方和： \mid\mid W^{[l]} \mid\mid_F^{2} = \sum_{i=1}^{n^{l-1}}\sum_{j=1}^{n^{l}}(w_{ij}^{[l]})^{2}<br>$$</p>
<h4 id="包含正则化项的梯度下降"><a href="#包含正则化项的梯度下降" class="headerlink" title="包含正则化项的梯度下降"></a>包含正则化项的梯度下降</h4><pre><code>collapsed:: true</code></pre><p>$$<br>              dW^{[l]} = (From Back Prop) + \frac{\lambda}{m}W^{[l]} \<br>              \text参数更新：W^{[l]} = W^{[l]} - \alpha \times [d(From Back Prop) + \frac{\lambda}{m}W^{[l]}] \<br>              = W^{[l]} - \alpha\frac{\lambda}{m}W^{[l]} - \alpha \times d(From Back Prop) \<br>              = (1-\alpha\frac{\lambda}{m})W^{[l]} - \alpha \times d(From Back Prop)  \tag1<br>$$<br>              从公式（1）可以看到，使用L2正则化后， $W^{[l]}$ 更新时先乘了一个小于 1 的系数 $1-\alpha\frac{\lambda}{m}$ ，由此L2正则化也被称作<strong>权重衰减</strong>。  </p>
<hr>
<h5 id="正则化预防过拟合原理"><a href="#正则化预防过拟合原理" class="headerlink" title="正则化预防过拟合原理"></a>正则化预防过拟合原理</h5><p>$$<br>$$<br>                    1）由公式（1）可直观理解，当 $\lambda$ 足够大，权重矩阵W被设置为接近于0的值，即把多隐藏单元的权重设置为0，极限情况下神经网络会变成一个小的网络，如同一个逻辑回归单元，可是深度却很大，这会使网络从过拟合状态更接近高偏差状态。<br>                      2）权重参数W越小，则Z值也越小，其所在的取值区间将偏向于激活函数（如： $\tanh$ ）的线性变换部分，这也能防止过拟合的发生。 </p>
<hr>
<h3 id="（二）Dropout（随机失活）"><a href="#（二）Dropout（随机失活）" class="headerlink" title="（二）Dropout（随机失活）"></a>（二）Dropout（随机失活）</h3><pre><code>collapsed:: true</code></pre><h4 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h4><p>遍历网络的每一层，设置消除神经网络节点的概率；消除一些节点，得到一个节点更少，规模更小的网络；用backprop方法进行训练。</p>
<h4 id="dropout实施"><a href="#dropout实施" class="headerlink" title="dropout实施"></a>dropout实施</h4><p>反向随机失活(Inverted dropout)<br>                1. 定义向量 $d3$ :<br>                  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>], a3.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><br>                1. d3 与 keep-prob 值比较，得到<strong>新的 d3</strong>，keep-prob在网络的不同层可以是不同的<br>                2. $a^{ [3]}$ 计算激活值<br>                  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 = np.multiply(a3, d3)</span><br></pre></td></tr></table></figure><br>                1. 扩展 $a^{ [3]} $以保证$ a^{ [3]} $期望值不变<br>                  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 /= keep-prob</span><br></pre></td></tr></table></figure><br>                1. 在<strong>测试阶段</strong> 不使用dropout，因为不期望输出结果是随机的。当keep-prob值为1时，保留所有神经元。</p>
<h4 id="dropout-预防过拟合原理"><a href="#dropout-预防过拟合原理" class="headerlink" title="dropout 预防过拟合原理"></a>dropout 预防过拟合原理</h4><pre><code>dropout 正则化相当于在每一次迭代过程都是训练一个以当前网络神经元子集所组成的一个新的简化的网络（越简单的网络越容易是欠拟合）。</code></pre><h4 id="dropout-所带来的问题"><a href="#dropout-所带来的问题" class="headerlink" title="dropout 所带来的问题"></a>dropout 所带来的问题</h4><pre><code>代价函数将不被明确定义，因为在每次迭代都会随机移除一些节点。</code></pre><h3 id="（三）其他正则化方法"><a href="#（三）其他正则化方法" class="headerlink" title="（三）其他正则化方法"></a>（三）其他正则化方法</h3><pre><code>collapsed:: true</code></pre><h4 id="1、数据扩增"><a href="#1、数据扩增" class="headerlink" title="1、数据扩增"></a>1、数据扩增</h4><h4 id="2、early-stoping"><a href="#2、early-stoping" class="headerlink" title="2、early stoping"></a>2、early stoping</h4><pre><code>early stoping就是在中间点停止迭代过程，得到一个w中等大小的佛罗贝尼乌斯范数。</code></pre><h2 id="过拟合-处理小结"><a href="#过拟合-处理小结" class="headerlink" title="过拟合 处理小结"></a>过拟合 处理小结</h2><pre><code>机器学习过程中需要解决的两个问题：  
1、代价函数优化  
解决这一问题的工具：梯度下降、Momentum、RMSProp、Adam。  
2、过拟合问题处理  
正则化、扩增数据等  </code></pre><p>$$<br>$$<br>            在优化代价函数时，只需要留意 $w 和 b, J(w,b)$ 值越小越好，过拟合问题即减少方差，使用另一套工具实现，这个原理被称为<strong>正交化</strong>。<br>            early stoping的主要 <strong>缺点：</strong> 是不能独立处理这两个问题，因为在提早停止了梯度下降也就结束优化代价函数； <strong>优点：</strong> 只运行一次梯度下降，就可以找出 $w$ 的较小值、中间值和较大值，而无需尝试 $L2$ 正则化超级参数的 $\lambda$ 的很多值。</p>
<h2 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h2><h3 id="1、归一化输入"><a href="#1、归一化输入" class="headerlink" title="1、归一化输入"></a>1、归一化输入</h3><p>$$<br>$$<br>          两个步骤：零均值、归一化方差<br>          无论是训练集还是测试集都是通过相同的 $\mu\text和\sigma^{2}$ 定义的数据转换，这两个是由训练集得出的。  </p>
<pre><code>* 零均值化</code></pre><p>$$<br>向量<br>\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)} \<br>x = x - \mu<br>$$<br>            * 归一化方差<br>$$<br>数据离散程度：\sigma^{2} = \frac{1}{m}\sum_{i=1}^{m}(x^{(i)})^{2} \<br>归一化：x = \frac{x}{\sigma^{2}}<br>$$</p>
<h3 id="2、参数初始化"><a href="#2、参数初始化" class="headerlink" title="2、参数初始化"></a>2、参数初始化</h3><pre><code>问题  

  梯度消失
  梯度爆炸  </code></pre><p>训练神经网络的时候，有时候梯度会变得非常大或非常小，这造成了训练的难度（难以收敛）  </p>
<p>解决办法<br>权重初始化，设置权重矩阵：<br>$$<br>w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}}) \<br>$$ 激活函数为 tanh，使用： $\sqrt{\frac{1}{n^{[l-1]}}}$ ，也被称为Xavier初始化。<br>激活函数为 Relu，使用： $\sqrt{\frac{2}{n^{[l-1]}}}$</p>
<h2 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h2><pre><code>用于发现 backprop  </code></pre><p>$$<br>      \text 1）计算gradapprox \<br>      1. \theta^{+} = \theta + \varepsilon\\<br>      2. \theta^{-} = \theta - \varepsilon\\<br>      3. J^{+} = J(\theta^{+})\\<br>      4. J^{-} = J(\theta^{-})\\<br>      5. gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon} \<br>      2) 通过反向传播，计算 grad<br>      3) 计算梯度差值：<br>$$</p>
<p>$$<br>      difference = \frac{\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2}   \tag{2}<br>$$<br>      如果差值较小（比如小于： $10^{-7}$ ）则可判断梯度计算正确。  </p>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><pre><code>目的：加快神经网络的运行。  </code></pre><h3 id="（一）mini-batch梯度下降"><a href="#（一）mini-batch梯度下降" class="headerlink" title="（一）mini-batch梯度下降"></a>（一）mini-batch梯度下降</h3><h4 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h4><p>$$<br>              \text 前向传播： Z^{[1]} = W^{[1]}X^{\{t\}} + b^{[1]}\\<br>              子集代价函数： J = \frac{1}{num}\sum_{i=1}^{num}L(\hat{y}^{(i)}, y^{(i)})\\<br>              正则化代价函数： J^{\{t\}} = \frac{1}{num}\sum_{i=1}^{num}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2\times{1000}}\sum_{l}\mid\mid{w^{[l]}\mid\mid}^{2}_{F} \\<br>              参数更新： W^{[l]} = W^{[l]} - \alpha \times{dW^{[l]}}\\<br>$$</p>
<pre><code>* 训练时，每个mini-batch都会进行一次完整的前线传播、反向传播、参数更新。在一个epoch内，实际上已经进行了多次参数更新。</code></pre><h4 id="batch-size-选择"><a href="#batch-size-选择" class="headerlink" title="batch size 选择"></a>batch size 选择</h4><pre><code>两个极端变种：  

  * mini-batch 大小等于m，其实就是batch梯度下降；
  * mini-batch 大小为1，则为随机梯度下降；  </code></pre><p>通常选择2的n次方，如：64、256、512。</p>
<h3 id="（二）基于指数加权平均的优化算法"><a href="#（二）基于指数加权平均的优化算法" class="headerlink" title="（二）基于指数加权平均的优化算法"></a>（二）基于指数加权平均的优化算法</h3><p>$$<br>          \text 指数加权平均数（指数加权移动平均数）：平均了 $\frac{1}{1-\beta}$ 个数据。<br>$$</p>
<p>$$<br>              v_{t} = \beta \times{v_{t-1}} + (1-\beta) \times \theta _{t}<br>$$<br>          偏差修正  </p>
<p>$$<br>          v_{t} = \frac{v_{t}}{1-\beta ^{t}}<br>$$<br>          下述的算法优化都是在正向传播、反向传播后得到参数的梯度值，在参数更新前对梯度值先进行平滑后再做参数更新。  </p>
<h3 id="1、动量梯度下降（Momentum）"><a href="#1、动量梯度下降（Momentum）" class="headerlink" title="1、动量梯度下降（Momentum）"></a>1、动量梯度下降（Momentum）</h3><p>$$<br>          \begin{cases}<br>          v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \<br>          W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}} \<br>          \end{cases}\tag{3}<br>$$<br>$$<br>          \begin{cases}<br>          v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \<br>          b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} \<br>          \end{cases}\tag{4} \<br>$$<br>          where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate.  </p>
<h3 id="2、RMSprop（Root-Mean-Square-prop）"><a href="#2、RMSprop（Root-Mean-Square-prop）" class="headerlink" title="2、RMSprop（Root Mean Square prop）"></a>2、RMSprop（Root Mean Square prop）</h3><p>$$<br>          \begin{cases}<br>          s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\<br>          W^{[l]} = W^{[l]} - \alpha \frac{dW}{\sqrt{s_{dW^{[l]}}}}\\<br>          s_{db^{[l]}} = \beta_2 s_{db^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial b^{[l]} })^2 \\<br>          b^{[l]} = b^{[l]} - \alpha \frac{db}{\sqrt{s_{db^{[l]}}}}<br>          \end{cases}<br>$$</p>
<h3 id="3、Adam-优化算法"><a href="#3、Adam-优化算法" class="headerlink" title="3、Adam 优化算法"></a>3、Adam 优化算法</h3><hr>
<p>$$<br>\begin{cases}<br>          v_{dW^{[l]}} &amp;= \beta_1 v_{dW^{[l]}} + (1 - \beta_1) (\frac{\partial \mathcal{J} }{\partial W^{[l]} }) \\<br>          v^c_{dW^{[l]}} &amp;= \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\<br>          s_{dW^{[l]}} &amp;= \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\<br>          s^c_{dW^{[l]}} &amp;= \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\<br>          W^{[l]} &amp;= W^{[l]} - \alpha (\frac{v^c_{dW^{[l]}}}{\sqrt{s^c_{dW^{[l]}}} + \varepsilon})<br>\end{cases}<br>$$    </p>
<p>$$<br>$$<br>          where:<br>            * t counts the number of steps taken of Adam<br>            * L is the number of layers<br>            * $\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages.<br>            * $\alpha$ is the learning rate<br>            * $\varepsilon$ is a very small number to avoid dividing by zero</p>
<h2 id="三-学习率"><a href="#三-学习率" class="headerlink" title="(三) 学习率"></a>(三) 学习率</h2><pre><code>指数衰减公式：  </code></pre><p>$$<br>      \alpha = 0.95^{epoch_num}  \alpha_0<br>$$<br>      其他衰减公式：  </p>
<p>$$<br>      \alpha = \frac{k}{\sqrt{epoch_num}} \alpha_0  \<br>      \alpha = \frac{k}{\sqrt{t}} \alpha_0  \<br>      \text (t为mini-batch数字）<br>$$    </p>
<h2 id="四-归一化网络激活函数-Batch-Normalization"><a href="#四-归一化网络激活函数-Batch-Normalization" class="headerlink" title="(四) 归一化网络激活函数(Batch Normalization)"></a>(四) 归一化网络激活函数(Batch Normalization)</h2><pre><code>* 类似于输入X，把l-1层的激活值当作l层的输入，归一化也可加快训练速度。与X归一化也存在差别：隐藏单元不必是平均值0和方差1。
* 隐藏层归一化选择在调用激活函数前，而归一化Z值，即： </code></pre><p>$$<br>a^{[l]} = g(\tilde{z}^{[l]})<br>$$</p>
<h3 id="隐藏层归一化公式"><a href="#隐藏层归一化公式" class="headerlink" title="隐藏层归一化公式"></a>隐藏层归一化公式</h3><p>$$<br>\mu = \frac{1}{m}\sum_{i=1}^{m}z^{[l](i)} \\<br>\sigma^{2} = \frac{1}{m}\sum_{i=1}^{m}(z^{[l](i)}-\mu)^{2} \\<br>z^{[l](i)}_{norm} = \frac{z^{[l](i)}-\mu}{\sqrt{\sigma^{2}+\epsilon}} \\<br>$$</p>
<p>$$<br>\text{上面公式完成了归一化，但隐藏单元并不一定想平均值为0，方差为1，下面公式中新参数决定了均值和方差}<br>$$</p>
<p>$$<br>\tilde{z}^{[l](i)} = \gamma z^{[l](i)}_{n} + \beta<br>$$</p>
<p>n是代表norm</p>
<p>$$<br>          \text l层参数增加两个： \beta 和 \gamma ，利用梯度下降进行优化:\\<br>          \beta^{[l]} = \beta^{[l]} - \alpha d\beta^{[l]} \\<br>          \gamma^{[l]} = \gamma^{[l]} - \alpha d\gamma^{[l]}<br>$$        </p>
<h3 id="归一化有效性原理"><a href="#归一化有效性原理" class="headerlink" title="归一化有效性原理"></a>归一化有效性原理</h3><pre><code>1）类似于对输入进行归一化操作后加快梯度下降速度的原理，对隐藏层进行归一化操作有着相似的作用；  
2）Batch 归一化减少了输入值改变的问题,它的确使这些值变得更稳定,神经网络的之后层就会有更坚实的基础。 即使使输入分布改变了一些, 它会改变得更少。 它做的是当前层保持学习,当改变时,迫使后层适应的程度减小了,你可以这样想,它减弱了前层参数的作用与后层参数的作用之间的联系, 它使得网络每层都可以自己学习, 稍稍独立于其它层, 这有助于加速整个网络的学习。  </code></pre><h3 id="Batch-归一化的使用"><a href="#Batch-归一化的使用" class="headerlink" title="Batch 归一化的使用"></a>Batch 归一化的使用</h3><h4 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h4><pre><code>Tensorflow
          <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.batch_normalization()</span><br></pre></td></tr></table></figure></code></pre><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><pre><code>训练时的Batch Norm  </code></pre><p>Batch归一化通常和训练集的mini-batch一起使用。<br>$$<br>\text 按 mini-batch(X^{[i]}) 为单位计算 z^{[l]}、\mu、\sigma^{2}<br>$$</p>
<pre><code>测试时的Batch Norm  </code></pre><p>问题：单个样本的测试如何获得 $\mu 和 \sigma$ ?<br>使用指数加权平均进行估算，这个平均数涵盖所有的mini-batch，计算过程：<br>                1. 计算每个mini-batch的每个隐藏层计算 $\mu^{\{i\}[l]}、\sigma^{2\{i\}[l]}$ ;<br>                2. 使用指数加权平均计算训练过程中得到的 $\mu^{\{i\}[l]}、\sigma^{2\{i\}[l]}$<br>                3. 计算 $z^{ i}_{ norm}、~{z} $</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>线形变换及激活<br>$$<br>Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\<br>t = e^{Z^{[l]}}<br>$$</p>
<p>$$<br>a^{[l]} = \frac{t_{i}}{\sum{_{i=1}^{n}}t_{i}}<br>$$</p>
<p>损失函数<br>$$<br>L(\hat{y}, y) = -\sum_{j=1}^{n}y_{j}\log\hat{y}_{j}<br>$$</p>
<p>代价函数<br>$$<br>J(w^{[l]}, b^{[l]}, … …) = \sum_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)})<br>$$<br>        * softmax 与 hardmax<br>Softmax 这个名称的来源是与所谓 hardmax 对比, hardmax 会把向量 z 变成这个向量[ 1 0 0 0 ], hardmax 函数会观察 z 的元素,然后在 z 中最大元素的位置放上 1,其它位置放上 0。<br>        * 反向传播<br>$$<br>\frac{\partial{L}}{\partial{z}} = dz^{[l]} = \hat{y} - y<br>$$</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/27/third-git-blog/" data-id="cllqdwhdb000h10h766xdcrx6" class="article-share-link" data-share="baidu" data-title="第二章 改善深层神经网络：超参数调试、正则化及优化">Share</a>
      

      
        <a href="http://example.com/2023/07/27/third-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-second-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/19/second-git-blog/" class="article-date">
  <time datetime="2023-07-19T08:36:59.000Z" itemprop="datePublished">2023-07-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/second-git-blog/">第一章 神经网络和机器学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="神经网络概要"><a href="#神经网络概要" class="headerlink" title="神经网络概要"></a>神经网络概要</h1><hr>
<p>首先吴恩达通过房价预测的例子，来形象的介绍了神经网络的作用。首先输入X，经过强大的神经网络，可将它映射成Y。<br>吴恩达在课程中也提到神经网络技术概念已经有几十年，为什么最近变得如此热门。笔者在05年研究生学习的时候，当时的课程已经是神经网络和机器学习，那个时候国内研究应用成果的还是比较少，但是理论的研究已经开始兴起，也是受国外很多期刊的影响，那个时候西安交大的徐宗本教授在此领域已经积累了不少经验和成果，后来凭借着这些成果，也当选成中科院院士，当时徐教授任笔者所在学院的院长，所以有幸也听过一些讲座。但随着后来没有从事相关领域的工作，到现在再重新拾起这门课，已经过了十几年的时光了。</p>
<p>回到刚才那个问题，为什么目前神经网络变得如此热门，吴恩达也提到这里面数据规模、计算量及算法的创新都功不可没。</p>
<h1 id="神经网络介绍"><a href="#神经网络介绍" class="headerlink" title="神经网络介绍"></a>神经网络介绍</h1><hr>
<p>为了使得神经网络的概念更好理解，吴恩达通过逻辑回归的例子来类比神经网络，使得我们通过一个简单的例子，了解神经网络的组成。以下图形很好的展示了神经网络的例子，此例子中的神经网络只包含了一个隐藏层。</p>
<p>  <img src="/2023/07/19/second-git-blog/1689757652003.png" alt="神经网络框架图"></p>
<p>  其中输入特征被竖直堆叠起来，叫做神经网络的输入层。另外一层称为隐藏层；最后一层只由一个结点构成，这个被称做输出层，它负责产生预测值。<br>  神经网络的三个要点描述为以下：</p>
<h3 id="输入（Input）-X"><a href="#输入（Input）-X" class="headerlink" title="输入（Input）:X"></a>输入（Input）:X</h3><h3 id="输出-（Output-Y"><a href="#输出-（Output-Y" class="headerlink" title="输出 （Output):Y"></a>输出 （Output):Y</h3><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><h3 id="激活函数-（Activation-Function）"><a href="#激活函数-（Activation-Function）" class="headerlink" title="激活函数 （Activation Function）"></a>激活函数 （Activation Function）</h3><pre><code>使用一个神经网络时，需要决定使用哪种激活函数用在隐藏层上，哪种用在输出节点上。文中介绍比较多的是sigmoid激活函数，实际应用中其他的激活函数效果会更好。激活函数实现了样本特征的非线形变换，如果没有非线形变换，可以证明即使有再多道隐层，神经网络也会“坍缩”成为逻辑回归。

常用激活函数：</code></pre><ul>
<li><p>Tanh<br>$$<br>g(z) = tanh(z)<br>=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \<br>g’(z) = 1 -(tanh(z))^{2}<br>$$</p>
</li>
<li><p>ReLU</p>
</li>
</ul>
<p>$$<br>g(z) = max(0, z) \<br>g’(z) = \begin{cases}<br>0 &amp; \text if  z \lt 0 \<br>1 &amp; \text if  z \ge 0<br>\end{cases}<br>$$</p>
<ul>
<li>Leaky ReLU</li>
</ul>
<p>$$<br>g(z) = max(0.01, z) \<br>g’(z) = \begin{cases}<br>0.01 &amp; \text if  z \lt 0 \<br>1 &amp; \text if z \ge 0<br>\end{cases}<br>$$</p>
<ul>
<li>sigmoid</li>
</ul>
<p>$$<br>g(z) = \frac{1}{1 + e^{-z}} \<br>g’(z) = g(z) \times (1 - g(z))<br>$$</p>
<ul>
<li>softmax</li>
</ul>
<p>$$<br>\text{for } x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}<br>      x_1  &amp;&amp;<br>      x_2 &amp;&amp;<br>      …  &amp;&amp;<br>      x_n<br>    \end{bmatrix}) =<br>     \begin{bmatrix}<br>       \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;<br>      \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;<br>      …  &amp;&amp;<br>      \frac{e^{x_n}}{\sum_{j}e^{x_j}}<br>    \end{bmatrix}  \<br>    \<br>$$</p>
<p>$$<br>softmax(x) = softmax<br>    \begin{bmatrix}<br>      x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \<br>      x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \<br>      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>      x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots  &amp; x_{mn}<br>    \end{bmatrix} =<br>    \begin{bmatrix}<br>      \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots  &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \<br>      \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots  &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \<br>      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>      \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots  &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}<br>    \end{bmatrix} \<br>  =<br>    \begin{pmatrix}<br>      softmax\text{(first row of x)}  \<br>      softmax\text{(second row of x)} \<br>      …  \<br>      softmax\text{(last row of x)} \<br>    \end{pmatrix}<br>$$</p>
<h3 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数 (Loss Function)"></a>损失函数 (Loss Function)</h3><pre><code>又叫误差函数，用来衡量算法的运行情况，通过这个L称为的损失函数，衡量预测输出值和实际值有多接近。一般用预测值和实际值的平方差或者它们平方差的一半。
  *   L1 损失函数</code></pre><p>$$<br>L_1(\hat{y},y) = \sum_{i=0}^m|(y^{(i)} - \hat{y}^{(i)})|<br>$$</p>
<pre><code>*   L2 损失函数</code></pre><p>$$<br>L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2<br>$$</p>
<pre><code>但通常逻辑回归中，会使用下面的L，</code></pre><p>$$<br>L(a, y) = -(ylog(a) + (1-y)log(1-a))<br>    L’(a,y) = \frac{1-y}{1-a} - \frac{y}{a}<br>$$</p>
<pre><code>这个的好处，以及这门课很多函数也和这个原理类似，就是如果y等于1，尽可能让$\hat&#123;y&#125;$变大，
如果y等于0，尽可能让$\hat&#123;y&#125;$变小</code></pre><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 (Cost Function)"></a>代价函数 (Cost Function)</h3><p>$$<br>\frac{1}{m}\sum_{i=1}^{m}L(a^{(i)}, y^{(i)})<br>$$</p>
<pre><code>损失函数()适用于单个训练样本，代价函数是参数的总代价</code></pre><ul>
<li>参数()：$w$, $b$*</li>
</ul>
<h2 id="逻辑回归与神经网络"><a href="#逻辑回归与神经网络" class="headerlink" title="逻辑回归与神经网络"></a>逻辑回归与神经网络</h2><hr>
<p>关于逻辑回归和神经网络类比，有以下的结论：其中逻辑回归可以看作是单层（没有隐藏层）的二分类神经网络，其中<br>  输入特征向量X，预测目标为0或1<br>  所以逻辑回归是给出输入x以及参数w和b之后，如何输出预测值$\hat{y}$。</p>
<p>  神经网络可以从两个方面看作是逻辑回归的叠加：</p>
<ul>
<li><p>层数的叠加<br>神经网络有多个隐层，每层有自己独立的参数。</p>
</li>
<li><p>特征参数的叠加<br>神经网络每一层在传入各层激活函数前，会进行多种不同的线形变换，每种变换经激活函数计算后形成该层一个神经元，一个神经元对应一次逻辑回归计算，同时也对应了参数矩阵一行。</p>
<p>以下是关于逻辑回归的训练目标与训练过程的介绍</p>
</li>
</ul>
<h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><hr>
<p>  逻辑回归作为二分类算法，其损失函数的数学意义可以理解为预测值$\hat{y}$为真实值$y$的概率，即：</p>
<p>$$<br>y = 1: p(y|x) =\hat{y}y = 0: p(y|x) = 1 - \hat{y}<br>$$</p>
<pre><code>将两个公式合并为一个公式：</code></pre><p>$$<br>p(y|x) = \hat{y}^{y}(1-\hat{y})^{(1-y)}<br>$$</p>
<pre><code>基于上述公式，我们需要 p(y|x) 所得概率值越大越好(从而达到预测值与真实值逼近的目的)。为避免进行指数运算所带来的复杂性，利用对数函数的严格单调性，对上述公式两边同时取对数，转换为下面的公式：</code></pre><p>$$<br>\log{p(y|x)} = y\log{\hat{y}} + (1-y)\log(1-\hat{y})<br>    = -L(\hat{y}, y)<br>$$</p>
<pre><code>概率值越大，损失值应该越小，二者应是相反关系且概率值都是小于1，其对数为负，所以损失值取负。
训练的目标就是通过寻找参数$w$, $b$,以获得最小的损失值。每次迭代时参数更新公式为：</code></pre><p>$$<br>w = w - \alpha\times dw<br>    b = b - \alpha\times db<br>$$</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><hr>
<p>   寻找最优参数过程基于凸函数的梯度下降()原理，通过对训练集迭代进行“正向传播()”和“反向传播()”完成。</p>
<h4 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h4><p>$$<br>    \text 根据样本数据 x 推导出 \hat{y},并保留中间值z用于反向传播的计算；<br>$$</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>$$<br>\frac{da}{dz} = a \times(1 - a)\\<br>    da = \frac{dL}{da} = \frac{1-y}{1-a} - \frac{y}{a} \\<br>    dz =\frac{dL}{dz} = \frac{dL}{da} \times \frac{da}{dz} = a - y\\<br>    \frac{dz}{dw} = x\\<br>    \frac{dz}{db} = 1 \\<br>    dw = \frac{dL}{dw} = \frac{dL}{dz} \times \frac{dz}{dw} = (a-y) \times x \\<br>    db = \frac{dL}{db} = \frac{dL}{dz} \times \frac{dz}{db} = a - y<br>$$</p>
<ul>
<li>多样本向量化计算<br>使用向量化计算比循环运算快约400倍。<br>$$<br>$$<br>输入为一个$（n，m）$的矩阵，m列的列向量，每个样本为$n$个特征值；<br>参数$w$维度为：（1，n),参数$b$维度为：（1，1)</li>
</ul>
<h2 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h2><h3 id="向量化运算"><a href="#向量化运算" class="headerlink" title="向量化运算"></a>向量化运算</h3><p>  向量在网络各层计算过程中的维度规律：<br>$$<br>$$<br>    * 各层样本个数保持不变，即：$A^{[i]}$、$Z^{[i]}$ 的列值均保持为m(样本数)，变化的是各层每个样本的特征数(行数)<br>    * 参数 $W^{[i]}$的维度与样本数无关，行为本层需生成的样本特征数，列为上层特征数；$B^{[i]}$为本层特征数，一列<br>    * 对于反向传播时的导数向量同样如此。</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><pre><code>#### 逻辑回归 参数初始化
  不存在中间隐层，输出层也只有一个神经元，参数可以初始化为任意值，考虑到激活函数曲线的斜率特性，通常都初始化为0。
#### 神经网络 参数初始化
  神经网络因隐层存在多个神经元，如参数初始化为0，则可证明每个神经元作用是等价的，因此，神经网络参数需初始化为随机值，且乘以一个较小的数，如：0.01，以使计算从斜率较大的地方开始。

* 另一项AI通用技术-数据归一化：训练数据归一化后将提高梯度下降的性能。举例, 如果：</code></pre><p>$$<br>x = \begin{bmatrix}<br>      0 &amp; 3 &amp; 4\<br>      2 &amp; 6 &amp; 4 \<br>      \end{bmatrix}<br>$$</p>
<p>then |x| = np.linalg.norm(x, axis = 1, keepdims = True)</p>
<p>$$<br>    = \begin{bmatrix}<br>      5\<br>      \sqrt{56} \<br>      \end{bmatrix}<br>$$</p>
<p>$$<br>    x_{normalized} = \frac{x}{|x|} =<br>      \begin{bmatrix}<br>      0 &amp; \frac{3}{5} &amp; \frac{4}{5} \<br>      \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \<br>      \end{bmatrix}<br>$$</p>
<h3 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h3><p>  上述介绍了神经网络的训练目标和训练过程，接下来我们来介绍，为什么要使用深层神经网络？<br>    过去实践经验，有些函数，只有非常深的神经网络能学会，而浅的模型办不到。<br>    同样课堂上将深度神经网络和人类大脑相比较，即先探测简单的东西（开始的层获取底层特征），组合起来才能探测复杂的物体（底层特征组合后的特征），从功能上讲，浅层网络也能达到同样效果，但是计算的效率较低。<br>    有两个特点<br>    small: 隐藏单元的数量相对较少<br>    deep: 隐藏层数目比较多<br>    深层网络隐藏单元数量相对较少，隐藏层数较多。如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<ul>
<li><p>参数      超参数：<br>$$<br>$$</p>
<p>算法中的学习率：learning rate $\alpha$<br>梯度下降循环的数量:n<br>隐藏层数目：L<br>隐藏层单元数目(): $n^[l]$<br>激活函数的选择<br>需要操作者自行设置的，这些数字实际上控制了最后的参数$w$, $b$，称之为超参数<br>按下来第二门课将会详细介绍参数的选择、正则化以及优化。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/19/second-git-blog/" data-id="cllqdwhda000g10h7eva8bdyt" class="article-share-link" data-share="baidu" data-title="第一章 神经网络和机器学习">Share</a>
      

      
        <a href="http://example.com/2023/07/19/second-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/18/hello-world/" class="article-date">
  <time datetime="2023-07-18T03:54:17.414Z" itemprop="datePublished">2023-07-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/18/hello-world/" data-id="cllqdwhd8000b10h70udy9zd8" class="article-share-link" data-share="baidu" data-title="Hello World">Share</a>
      

      
        <a href="http://example.com/2023/07/18/hello-world/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  
    <article id="post-first-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/17/first-git-blog/" class="article-date">
  <time datetime="2023-07-17T08:36:59.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/first-git-blog/">前言</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="写在博客前面"><a href="#写在博客前面" class="headerlink" title="写在博客前面"></a><strong>写在博客前面</strong></h1><hr>
<p>前后差不多将近一年半的时间（从2021年的10月份从网上看到深度学习的经典之作开始学习），比较曲折，耗时较长，中间还经历过上海封控等因素（2022.3-6），同时大家因为也有正常的工作要做，留给深度学习的时间比较零碎，但其实去年的时候（2022.5），已经完成了主体部分的学习。后来计划是写些东西，首先要整理笔记，把之前各门科的笔记和对应作业的要点整理起来，差不多在今年过年时候完成（2023.2）。然后这个写作的任务就落在我的头上，加上今年公司的改革，我的工作上半年特别忙碌，所以笔记拖到现(2023.5)在。最近是两个项目的空档时间，抓紧利用这个时间可以快速把之前的学习经历、心得和作业要点通通梳理，也算是为这次学习画上完满的句号。之后将是针对所学内容的开发工具或者相关算法的改进，这个是后话，也希望学到的东西可以持续的应用起来。</p>
<h1 id="为什么选择这套教材"><a href="#为什么选择这套教材" class="headerlink" title="为什么选择这套教材"></a><strong>为什么选择这套教材</strong></h1><hr>
<p>深度挖掘是当今科技界比较热门的技能，对于想入门的同学，除了要掌握一般的编程知识，熟悉机器学习的基本概念，还需要有python编程的基础。但即使掌握了这些技能，如何选择一门好的入门课程仍然困扰着大家。例如笔者一开始的时候，网上购买了一些付费课程，但是网上的课程鱼龙混杂，跟着视频学习的时候，发现遇到了更多的问题，另外课程上存在着各种只是简单的复制，没有注重课程的连续性，或者是其中一些必要的知识，没有介绍清楚，导致学习起来特别痛苦。花了很多时间去调查中间的过程。<br>在中间探索的过程中，发现了吴恩达YYDS的深度挖掘的课程，但是由于笔者和朋友已经不接触英语好久了，对于全英文的课程，心里还是存着敬畏。感觉没有勇气能够坚持完全部课程的学习。后来发现了已经翻译成中文的学习笔记和网易云课程已经将视频做了翻译，这样就扫清了前期学习的语言障碍。<br>吴恩达这些课程，总共有5堂课，通过课程的学习，可以学习到深度学习的基础，学会构建神经网络。并且可以在吴恩达本人及多位业务顶尖专家的指导下创建自己的机器 学习项目。其中对卷积神经网络(CNN)、递归神经网络（RNN）、长短期记忆（LSTM）等深度学习都有所涉及。每节课都配有习题和编程练习，之后我们将会分章节，解析习题和编程练习，以便大家可以更好地了解深度挖掘的原理和应用。</p>
<h1 id="章节内容介绍"><a href="#章节内容介绍" class="headerlink" title="章节内容介绍"></a><strong>章节内容介绍</strong></h1><hr>
<p>主要是包括五门课的内容，主要的内容和结构如下:<br>文章的脉落也比较清晰，第一门课是对于神经网络用到的基础进行阐述，同时介绍了浅层网络和深层网络的基础，在大家对于这些概念有些基本了解后，开始进行深入探索。在第二门课时，对于深层神经网络的参数调节、正则化以及优化做了详细的阐述，紧接着是第三门课对于结构化机器学习项目的学习策略进行介绍，主要是基于作者在日常的项目中遇到的问题以及大家经常关心的策略等有了比较详细的介绍。之后在第四门课时介绍了卷积神经网络，以及目标检测的应用，通过两个特殊应用：人脸识别和神经风格的转换，介绍了卷积神经网络的应用。最后一门课第五门课是关于序列模型的介绍，这块对于目前自然语言的应用等领域有着非常好的借鉴作用。</p>
<h2 id="章节列表"><a href="#章节列表" class="headerlink" title="章节列表"></a><strong>章节列表</strong></h2><hr>
<h3 id="第一门课-神经网络和深度学习"><a href="#第一门课-神经网络和深度学习" class="headerlink" title="第一门课  神经网络和深度学习"></a>第一门课  神经网络和深度学习</h3><pre><code>- 深度学习引言
- 神经网络的编程基础
- 浅层神经网络
- 深层神经网络</code></pre><h3 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化" class="headerlink" title="第二门课  改善深层神经网络：超参数调试、正则化以及优化"></a>第二门课  改善深层神经网络：超参数调试、正则化以及优化</h3><pre><code>- 深度学习的实践层面
- 优化算法
- 超参数调试、Batch正则化和程序框架</code></pre><h3 id="第三门课-结构化机器学习项目"><a href="#第三门课-结构化机器学习项目" class="headerlink" title="第三门课 结构化机器学习项目"></a>第三门课 结构化机器学习项目</h3><pre><code>- 机器学习策略1
- 机器学习策略2</code></pre><h3 id="第四门课-卷积神经网络"><a href="#第四门课-卷积神经网络" class="headerlink" title="第四门课  卷积神经网络"></a>第四门课  卷积神经网络</h3><pre><code>- 卷积神经网络
- 深度卷积网络
- 目标检测
- 特殊应用：人脸识别和神经风格转换</code></pre><h3 id="第五门课-序列模型"><a href="#第五门课-序列模型" class="headerlink" title="第五门课  序列模型"></a>第五门课  序列模型</h3><pre><code>- 循环序列模型
- 自然语言处理与词嵌入
- 序列模型和注意力机制</code></pre>
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/17/first-git-blog/" data-id="cllqdwhd1000110h71gwd2k3p" class="article-share-link" data-share="baidu" data-title="前言">Share</a>
      

      
        <a href="http://example.com/2023/07/17/first-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  
    <article id="post-my-first-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/17/my-first-blog/" class="article-date">
  <time datetime="2023-07-17T07:00:58.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/my-first-blog/">Hexo基本使用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hexo的优点：快速、简洁且高效的博客框架。Hexo使用Markdown解析文章，在几秒内，即可利用按照指定主题生成静态网页。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装使用hexo之前需要先安装Node.js和Git，当已经安装了Node.js和npm(npm是node.js的包管理工具)，可以通过以下命令安装hexo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p>可以通过以下命令查看主机中是否安装了node.js和npm</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ node --version    <span class="comment">#检查是否安装了node.js</span></span><br><span class="line">$ npm --version     <span class="comment">#检查是否安装了npm</span></span><br></pre></td></tr></table></figure>

<p>如果已经安装，提示如下：</p>
<p>$ C:\Users\YingZhou\blog&gt;node –version<br>v18.16.1</p>
<p>C:\Users\YingZhou\blog&gt;npm –version<br>9.5.1</p>
<h1 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h1><p>安装完Hexo之后，执行下列命令，Hexo将会在指定目录中新建所需要的文件，指定的目录即为Hexo的工作站</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>


<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/17/my-first-blog/" data-id="cllqdwhd9000d10h7b62l3tfs" class="article-share-link" data-share="baidu" data-title="Hexo基本使用">Share</a>
      

      
        <a href="http://example.com/2023/07/17/my-first-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  

</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82/">参数调节</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%95%AA%E5%A4%96%E7%AF%87/" rel="tag">番外篇</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" style="font-size: 20px;">深度挖掘</a> <a href="/tags/%E7%95%AA%E5%A4%96%E7%AF%87/" style="font-size: 10px;">番外篇</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">6</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/08/15/fifth-git-blog/">第四章：卷积神经网络</a>
          </li>
        
          <li>
            <a href="/2023/08/05/four-git-blog/">第三章：结构化机器学习项目</a>
          </li>
        
          <li>
            <a href="/2023/07/27/hexo-equation-blog/">Hexo如何解决公式渲染问题</a>
          </li>
        
          <li>
            <a href="/2023/07/27/third-git-blog/">第二章 改善深层神经网络：超参数调试、正则化及优化</a>
          </li>
        
          <li>
            <a href="/2023/07/19/second-git-blog/">第一章 神经网络和机器学习</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://arvinxiang.com" target="_blank">主题作者</a>
          </li>
        
          <li>
            <a href="http://reqianduan.com" target="_blank">热前端</a>
          </li>
        
          <li>
            <a href="http://yuancheng.work" target="_blank">远程.work</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
