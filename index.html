
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>熊妞随手记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="熊妞随手记">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="熊妞随手记">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="熊妞随手记" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">熊妞随手记</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="example.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main">
  
    <article id="post-hexo-equation-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/27/hexo-equation-blog/" class="article-date">
  <time datetime="2023-07-27T08:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hexo/">Hexo</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/hexo-equation-blog/">Hexo如何解决公式渲染问题</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>  找到Hexo可以搭建博客框架，着实高兴了好几天，感觉以后可以把各种深度挖掘研究的笔记转移到这个战场了。但真正开始使用，发现了好多问题，其中最大的一个问题是公式渲染问题。网上找了各种资料，例如将现在hexo-renderer-marked换成hexo-renderer-kramed，这个我也有尝试，对于行间的公式渲染没有问题了，但是这个无法解决的是行内的公式，也没有找到特别好的办法。</p>
<h2 id="方法简介"><a href="#方法简介" class="headerlink" title="方法简介"></a>方法简介</h2><p>   可能因为文章确实存在很久没有更新的问题，所以得到的结果并不能有效解决当前所有的问题。<br>这个时间正好看到有篇文章提到，还是继续使用marked,只是换个版本的解决方案，这个解决了至少大多数的问题，具体的解决方式如下：</p>
<ul>
<li>打开hexo blog 目录：<br>cmd blog</li>
<li>卸载原先安装的marked<br>npm uninstall hexo-renderer-marked</li>
<li>重新安装marked 1.0.0版本<br>npm install <a href="mailto:hexo-rnderer-marked@1.0.0">hexo-rnderer-marked@1.0.0</a> </li>
<li>修改marked.js配置<br>编辑node_modules/marked/lib/marked.js<br>将第539行的<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([!&quot;#$%&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]\\^_`&#123;|&#125;~])/&#125; </span><br><span class="line">改为 </span><br><span class="line">escape: /^\\([!&quot;#$%&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]\\^_`&#123;|&#125;~])/&#125;)</span><br><span class="line">将第564行的 </span><br><span class="line">inline._escapes = /\\([!&quot;#$%&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]\\^_`&#123;|&#125;~])/g;</span><br><span class="line">改为:</span><br><span class="line">inline._escapes = /\\([!&quot;#$&amp;&#x27;()*+,\-./:;&lt;=&gt;?@\[\]^_`|~])/g;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>以上操作应该可以解决大多数公式渲染问题。</p>
<p>当然我也遇到过几个比较特殊的问题，也给大家分享下：<br>例如某个公式里面要用到<em>，可能由于一些原因，不能正确渲染，这时候建议在前面加上\，变为\</em><br>另外对于\frac公式，可以通过后面加上()等形式来解决。</p>
<p>针对特殊的符号渲染，目前的解决方案，只能像是调试程序一样，先一点点去定位问题，然后再去找到是什么问题造成的。对于不能渲染的符号，一般都可通过加\来达到目的，但是首先要知道是对于什么符号不能有效识别。</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/27/hexo-equation-blog/" data-id="clkkvbagf00027kh7gj7hd0hc" class="article-share-link" data-share="baidu" data-title="Hexo如何解决公式渲染问题">Share</a>
      

      
        <a href="http://example.com/2023/07/27/hexo-equation-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%95%AA%E5%A4%96%E7%AF%87/" rel="tag">番外篇</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-third-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/27/third-git-blog/" class="article-date">
  <time datetime="2023-07-27T08:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82/">参数调节</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/third-git-blog/">第二章 改善深层神经网络：超参数调试、正则化及优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="改善深层神经网络：超参数调试、正则化及优化"><a href="#改善深层神经网络：超参数调试、正则化及优化" class="headerlink" title="改善深层神经网络：超参数调试、正则化及优化"></a>改善深层神经网络：超参数调试、正则化及优化</h1><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>  通过第一门课的学习，我们对于神经网络的基本概念，逻辑回归和神经网络的类比，以及浅层和深层神经网络有了了解。这门课针对深层神经网络的超参数调试、正则化以及优化做进一步介绍，以便在实际应用的时候，对于超参数的选择，提供了很多方法和例子，加深我们的理解。有效的运作神经网络主要包括：超参数调优、如何构建数据、如何确保优化算法正常运行，从而使学习算法在合理时间内完成自我学习。  </p>
<h1 id="详情"><a href="#详情" class="headerlink" title="详情"></a>详情</h1><hr>
<h2 id="构建数据"><a href="#构建数据" class="headerlink" title="构建数据"></a>构建数据</h2><table>
    <tr>
        <td></td> <td>功能</td> <td>数据来源</td>  <td>小数据量</td> <td>大数据量</td>
    </tr>
    <tr>
        <td>训练集</td> <td>训练模型</td> <td></td>  <td>70%</td> <td>98%</td>
    </tr>
    <tr>
        <td>验证集</td> <td>选择模型</td> <td>保证与测试集同分布</td>  <td>&lt;10%~20%</td> <td>1%</td>
    </tr>
    <tr>
        <td>测试集</td> <td>评估模型</td> <td>保证与验证集同分布</td>  <td>&lt;10%~20%</td> <td>1%</td>
    </tr>
</table>

<h2 id="偏差误差"><a href="#偏差误差" class="headerlink" title="偏差误差"></a>偏差误差</h2><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><pre><code>* 初始模型训练完成后，评估算法的偏差高不高  </code></pre><p>偏差 高：评估训练集或训练数据的性能；</p>
<hr>
<p>如果无法拟合训练集，则需要<strong>选择一个新网络</strong>或者<strong>花更多时间训练网络</strong>或者<strong>尝试更先进的优化算法</strong>。<br>本阶段需达成的目标是：<strong>拟合训练集</strong>。<br>            * <strong>偏差</strong>降低到可接受程度后进行<strong>方差</strong>评估<br><strong>方差</strong>高：最好的办法是<strong>采用更多的数据</strong>；当无法获得更多数据的时候，采用<strong>正则化</strong>来减少过拟合。<br>            * 最终目标  ：<strong>低偏差、低方差</strong> 框架。</p>
<h3 id="偏差分析"><a href="#偏差分析" class="headerlink" title="偏差分析"></a>偏差分析</h3><pre><code>collapsed:: true
神经网络训练的流程显示模型训练的目标是**低偏差、低方差**，需依据偏差、方差的高低调整训练策略。  

  * **偏差：** 训练集误差与 **最优误差（贝叶斯误差）** 的比较结果；  </code></pre><p>最优误差：通常采用人的识别误差，如误差为0。<br>            * <strong>方差：</strong>验证集误差与 <strong>最优误差（贝叶斯误差）</strong> 的比较结果；</p>
<table>
<tr>
<td>误差分析</td> <td>特征</td> <td>处理</td>
</tr>
<tr>
<td>欠拟合</td> <td>偏差大：训练集误差大于最优误差  </td> <td><br>选择新网络<br><br>延长训练时间<br><br>尝试新的优化算法<br></td>
</tr>
<tr>
<td>过拟合</td> <td>方差大：验证集误差大于最优误差；<br>偏差小<br></td> <td><br>采用更多的数据训练<br><br>正则化<br></td>
</tr>
</table>

<h4 id="欠拟合问题处理"><a href="#欠拟合问题处理" class="headerlink" title="欠拟合问题处理"></a>欠拟合问题处理</h4><pre><code>消除欠拟合是模型训练的第一步目标，至少要拟合训练集，如无法达到则说明“方向”性错误。  </code></pre><h4 id="过拟合问题处理"><a href="#过拟合问题处理" class="headerlink" title="过拟合问题处理"></a>过拟合问题处理</h4><pre><code>* 更多的训练数据
* 正则化（当无法获取更多数据时）
* 数据生成</code></pre><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="（一）L2正则化"><a href="#（一）L2正则化" class="headerlink" title="（一）L2正则化"></a>（一）L2正则化</h3><h4 id="1、逻辑回归的正则化"><a href="#1、逻辑回归的正则化" class="headerlink" title="1、逻辑回归的正则化"></a>1、逻辑回归的正则化</h4><p>$$<br>              \text L2正则化：<br>              J(W, b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\mid\mid W \mid\mid_2^{2} \<br>              \text 向量参数 W 的 L2范数： \mid\mid W \mid\mid_2^{2} = W^{T}W = \sum_{j=1}^{n_{x}}w_j^{2} \<br>              W\in \mathbb{R}^{n_x}, b\in\mathbb{R} \<br>              \<br>              \text L1正则化：<br>              J(W, b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\mid\mid W \mid\mid_1 \<br>              \text 向量参数 W 的 L1范数： \mid\mid W \mid\mid_1 = \sum_{j=1}^{n_{x}}\mid w_j\mid \\<br>$$          </p>
<h4 id="2、神经网络的正则化"><a href="#2、神经网络的正则化" class="headerlink" title="2、神经网络的正则化"></a>2、神经网络的正则化</h4><p>$$<br>              \text L2正则化：<br>              J(W^{[1]}, b^{[1]},…,W^{[L]}, b^{[L]}) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y) + \frac{\lambda}{2m}\sum_{l=1}^L\mid\mid W^{[l]} \mid\mid_F^{2} \<br>              \text 矩阵范数（费罗贝尼乌斯范数）被定义为矩阵中所有元素的平方和： \mid\mid W^{[l]} \mid\mid_F^{2} = \sum_{i=1}^{n^{l-1}}\sum_{j=1}^{n^{l}}(w_{ij}^{[l]})^{2}<br>$$</p>
<h4 id="包含正则化项的梯度下降"><a href="#包含正则化项的梯度下降" class="headerlink" title="包含正则化项的梯度下降"></a>包含正则化项的梯度下降</h4><pre><code>collapsed:: true</code></pre><p>$$<br>              dW^{[l]} = (From Back Prop) + \frac{\lambda}{m}W^{[l]} \<br>              \text参数更新：W^{[l]} = W^{[l]} - \alpha \times [d(From Back Prop) + \frac{\lambda}{m}W^{[l]}] \<br>              = W^{[l]} - \alpha\frac{\lambda}{m}W^{[l]} - \alpha \times d(From Back Prop) \<br>              = (1-\alpha\frac{\lambda}{m})W^{[l]} - \alpha \times d(From Back Prop)  \tag1<br>$$<br>              从公式（1）可以看到，使用L2正则化后， $W^{[l]}$ 更新时先乘了一个小于 1 的系数 $1-\alpha\frac{\lambda}{m}$ ，由此L2正则化也被称作<strong>权重衰减</strong>。  </p>
<hr>
<h5 id="正则化预防过拟合原理"><a href="#正则化预防过拟合原理" class="headerlink" title="正则化预防过拟合原理"></a>正则化预防过拟合原理</h5><p>$$<br>$$<br>                    1）由公式（1）可直观理解，当 $\lambda$ 足够大，权重矩阵W被设置为接近于0的值，即把多隐藏单元的权重设置为0，极限情况下神经网络会变成一个小的网络，如同一个逻辑回归单元，可是深度却很大，这会使网络从过拟合状态更接近高偏差状态。<br>                      2）权重参数W越小，则Z值也越小，其所在的取值区间将偏向于激活函数（如： $\tanh$ ）的线性变换部分，这也能防止过拟合的发生。 </p>
<hr>
<h3 id="（二）Dropout（随机失活）"><a href="#（二）Dropout（随机失活）" class="headerlink" title="（二）Dropout（随机失活）"></a>（二）Dropout（随机失活）</h3><pre><code>collapsed:: true</code></pre><h4 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h4><p>遍历网络的每一层，设置消除神经网络节点的概率；消除一些节点，得到一个节点更少，规模更小的网络；用backprop方法进行训练。</p>
<h4 id="dropout实施"><a href="#dropout实施" class="headerlink" title="dropout实施"></a>dropout实施</h4><p>反向随机失活(Inverted dropout)<br>                1. 定义向量 $d3$ :<br>                  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>], a3.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><br>                1. d3 与 keep-prob 值比较，得到<strong>新的 d3</strong>，keep-prob在网络的不同层可以是不同的<br>                2. $a^{ [3]}$ 计算激活值<br>                  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 = np.multiply(a3, d3)</span><br></pre></td></tr></table></figure><br>                1. 扩展 $a^{ [3]} $以保证$ a^{ [3]} $期望值不变<br>                  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 /= keep-prob</span><br></pre></td></tr></table></figure><br>                1. 在<strong>测试阶段</strong> 不使用dropout，因为不期望输出结果是随机的。当keep-prob值为1时，保留所有神经元。</p>
<h4 id="dropout-预防过拟合原理"><a href="#dropout-预防过拟合原理" class="headerlink" title="dropout 预防过拟合原理"></a>dropout 预防过拟合原理</h4><pre><code>dropout 正则化相当于在每一次迭代过程都是训练一个以当前网络神经元子集所组成的一个新的简化的网络（越简单的网络越容易是欠拟合）。</code></pre><h4 id="dropout-所带来的问题"><a href="#dropout-所带来的问题" class="headerlink" title="dropout 所带来的问题"></a>dropout 所带来的问题</h4><pre><code>代价函数将不被明确定义，因为在每次迭代都会随机移除一些节点。</code></pre><h3 id="（三）其他正则化方法"><a href="#（三）其他正则化方法" class="headerlink" title="（三）其他正则化方法"></a>（三）其他正则化方法</h3><pre><code>collapsed:: true</code></pre><h4 id="1、数据扩增"><a href="#1、数据扩增" class="headerlink" title="1、数据扩增"></a>1、数据扩增</h4><h4 id="2、early-stoping"><a href="#2、early-stoping" class="headerlink" title="2、early stoping"></a>2、early stoping</h4><pre><code>early stoping就是在中间点停止迭代过程，得到一个w中等大小的佛罗贝尼乌斯范数。</code></pre><h2 id="过拟合-处理小结"><a href="#过拟合-处理小结" class="headerlink" title="过拟合 处理小结"></a>过拟合 处理小结</h2><pre><code>机器学习过程中需要解决的两个问题：  
1、代价函数优化  
解决这一问题的工具：梯度下降、Momentum、RMSProp、Adam。  
2、过拟合问题处理  
正则化、扩增数据等  </code></pre><p>$$<br>$$<br>            在优化代价函数时，只需要留意 $w 和 b, J(w,b)$ 值越小越好，过拟合问题即减少方差，使用另一套工具实现，这个原理被称为<strong>正交化</strong>。<br>            early stoping的主要 <strong>缺点：</strong> 是不能独立处理这两个问题，因为在提早停止了梯度下降也就结束优化代价函数； <strong>优点：</strong> 只运行一次梯度下降，就可以找出 $w$ 的较小值、中间值和较大值，而无需尝试 $L2$ 正则化超级参数的 $\lambda$ 的很多值。</p>
<h2 id="加速训练"><a href="#加速训练" class="headerlink" title="加速训练"></a>加速训练</h2><h3 id="1、归一化输入"><a href="#1、归一化输入" class="headerlink" title="1、归一化输入"></a>1、归一化输入</h3><p>$$<br>$$<br>          两个步骤：零均值、归一化方差<br>          无论是训练集还是测试集都是通过相同的 $\mu\text和\sigma^{2}$ 定义的数据转换，这两个是由训练集得出的。  </p>
<pre><code>* 零均值化</code></pre><p>$$<br>向量<br>\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)} \<br>x = x - \mu<br>$$<br>            * 归一化方差<br>$$<br>数据离散程度：\sigma^{2} = \frac{1}{m}\sum_{i=1}^{m}(x^{(i)})^{2} \<br>归一化：x = \frac{x}{\sigma^{2}}<br>$$</p>
<h3 id="2、参数初始化"><a href="#2、参数初始化" class="headerlink" title="2、参数初始化"></a>2、参数初始化</h3><pre><code>问题  

  梯度消失
  梯度爆炸  </code></pre><p>训练神经网络的时候，有时候梯度会变得非常大或非常小，这造成了训练的难度（难以收敛）  </p>
<p>解决办法<br>权重初始化，设置权重矩阵：<br>$$<br>w^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}}) \<br>$$ 激活函数为 tanh，使用： $\sqrt{\frac{1}{n^{[l-1]}}}$ ，也被称为Xavier初始化。<br>激活函数为 Relu，使用： $\sqrt{\frac{2}{n^{[l-1]}}}$</p>
<h2 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h2><pre><code>用于发现 backprop  </code></pre><p>$$<br>      \text 1）计算gradapprox \<br>      1. \theta^{+} = \theta + \varepsilon\\<br>      2. \theta^{-} = \theta - \varepsilon\\<br>      3. J^{+} = J(\theta^{+})\\<br>      4. J^{-} = J(\theta^{-})\\<br>      5. gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon} \<br>      2) 通过反向传播，计算 grad<br>      3) 计算梯度差值：<br>$$</p>
<p>$$<br>      difference = \frac{\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2}   \tag{2}<br>$$<br>      如果差值较小（比如小于： $10^{-7}$ ）则可判断梯度计算正确。  </p>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><pre><code>目的：加快神经网络的运行。  </code></pre><h3 id="（一）mini-batch梯度下降"><a href="#（一）mini-batch梯度下降" class="headerlink" title="（一）mini-batch梯度下降"></a>（一）mini-batch梯度下降</h3><h4 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h4><p>$$<br>              \text 前向传播： Z^{[1]} = W^{[1]}X^{\{t\}} + b^{[1]}\\<br>              子集代价函数： J = \frac{1}{num}\sum_{i=1}^{num}L(\hat{y}^{(i)}, y^{(i)})\\<br>              正则化代价函数： J^{\{t\}} = \frac{1}{num}\sum_{i=1}^{num}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2\times{1000}}\sum_{l}\mid\mid{w^{[l]}\mid\mid}^{2}_{F} \\<br>              参数更新： W^{[l]} = W^{[l]} - \alpha \times{dW^{[l]}}\\<br>$$</p>
<pre><code>* 训练时，每个mini-batch都会进行一次完整的前线传播、反向传播、参数更新。在一个epoch内，实际上已经进行了多次参数更新。</code></pre><h4 id="batch-size-选择"><a href="#batch-size-选择" class="headerlink" title="batch size 选择"></a>batch size 选择</h4><pre><code>两个极端变种：  

  * mini-batch 大小等于m，其实就是batch梯度下降；
  * mini-batch 大小为1，则为随机梯度下降；  </code></pre><p>通常选择2的n次方，如：64、256、512。</p>
<h3 id="（二）基于指数加权平均的优化算法"><a href="#（二）基于指数加权平均的优化算法" class="headerlink" title="（二）基于指数加权平均的优化算法"></a>（二）基于指数加权平均的优化算法</h3><p>$$<br>          \text 指数加权平均数（指数加权移动平均数）：平均了 $\frac{1}{1-\beta}$ 个数据。<br>$$</p>
<p>$$<br>              v_{t} = \beta \times{v_{t-1}} + (1-\beta) \times \theta _{t}<br>$$<br>          偏差修正  </p>
<p>$$<br>          v_{t} = \frac{v_{t}}{1-\beta ^{t}}<br>$$<br>          下述的算法优化都是在正向传播、反向传播后得到参数的梯度值，在参数更新前对梯度值先进行平滑后再做参数更新。  </p>
<h3 id="1、动量梯度下降（Momentum）"><a href="#1、动量梯度下降（Momentum）" class="headerlink" title="1、动量梯度下降（Momentum）"></a>1、动量梯度下降（Momentum）</h3><p>$$<br>          \begin{cases}<br>          v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \<br>          W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}} \<br>          \end{cases}\tag{3}<br>$$<br>$$<br>          \begin{cases}<br>          v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \<br>          b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} \<br>          \end{cases}\tag{4} \<br>$$<br>          where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate.  </p>
<h3 id="2、RMSprop（Root-Mean-Square-prop）"><a href="#2、RMSprop（Root-Mean-Square-prop）" class="headerlink" title="2、RMSprop（Root Mean Square prop）"></a>2、RMSprop（Root Mean Square prop）</h3><p>$$<br>          \begin{cases}<br>          s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\<br>          W^{[l]} = W^{[l]} - \alpha \frac{dW}{\sqrt{s_{dW^{[l]}}}}\\<br>          s_{db^{[l]}} = \beta_2 s_{db^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial b^{[l]} })^2 \\<br>          b^{[l]} = b^{[l]} - \alpha \frac{db}{\sqrt{s_{db^{[l]}}}}<br>          \end{cases}<br>$$</p>
<h3 id="3、Adam-优化算法"><a href="#3、Adam-优化算法" class="headerlink" title="3、Adam 优化算法"></a>3、Adam 优化算法</h3><hr>
<p>$$<br>\begin{cases}<br>          v_{dW^{[l]}} &amp;= \beta_1 v_{dW^{[l]}} + (1 - \beta_1) (\frac{\partial \mathcal{J} }{\partial W^{[l]} }) \\<br>          v^c_{dW^{[l]}} &amp;= \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\<br>          s_{dW^{[l]}} &amp;= \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\<br>          s^c_{dW^{[l]}} &amp;= \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\<br>          W^{[l]} &amp;= W^{[l]} - \alpha (\frac{v^c_{dW^{[l]}}}{\sqrt{s^c_{dW^{[l]}}} + \varepsilon})<br>\end{cases}<br>$$    </p>
<p>$$<br>$$<br>          where:<br>            * t counts the number of steps taken of Adam<br>            * L is the number of layers<br>            * $\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages.<br>            * $\alpha$ is the learning rate<br>            * $\varepsilon$ is a very small number to avoid dividing by zero</p>
<h2 id="三-学习率"><a href="#三-学习率" class="headerlink" title="(三) 学习率"></a>(三) 学习率</h2><pre><code>指数衰减公式：  </code></pre><p>$$<br>      \alpha = 0.95^{epoch_num}  \alpha_0<br>$$<br>      其他衰减公式：  </p>
<p>$$<br>      \alpha = \frac{k}{\sqrt{epoch_num}} \alpha_0  \<br>      \alpha = \frac{k}{\sqrt{t}} \alpha_0  \<br>      \text (t为mini-batch数字）<br>$$    </p>
<h2 id="四-归一化网络激活函数-Batch-Normalization"><a href="#四-归一化网络激活函数-Batch-Normalization" class="headerlink" title="(四) 归一化网络激活函数(Batch Normalization)"></a>(四) 归一化网络激活函数(Batch Normalization)</h2><pre><code>* 类似于输入X，把l-1层的激活值当作l层的输入，归一化也可加快训练速度。与X归一化也存在差别：隐藏单元不必是平均值0和方差1。
* 隐藏层归一化选择在调用激活函数前，而归一化Z值，即： </code></pre><p>$$<br>a^{[l]} = g(\tilde{z}^{[l]})<br>$$</p>
<h3 id="隐藏层归一化公式"><a href="#隐藏层归一化公式" class="headerlink" title="隐藏层归一化公式"></a>隐藏层归一化公式</h3><p>$$<br>\mu = \frac{1}{m}\sum_{i=1}^{m}z^{[l](i)} \\<br>\sigma^{2} = \frac{1}{m}\sum_{i=1}^{m}(z^{[l](i)}-\mu)^{2} \\<br>z^{[l](i)}_{norm} = \frac{z^{[l](i)}-\mu}{\sqrt{\sigma^{2}+\epsilon}} \\<br>$$</p>
<p>$$<br>\text{上面公式完成了归一化，但隐藏单元并不一定想平均值为0，方差为1，下面公式中新参数决定了均值和方差}<br>$$</p>
<p>$$<br>\tilde{z}^{[l](i)} = \gamma z^{[l](i)}_{n} + \beta<br>$$</p>
<p>n是代表norm</p>
<p>$$<br>          \text l层参数增加两个： \beta 和 \gamma ，利用梯度下降进行优化:\\<br>          \beta^{[l]} = \beta^{[l]} - \alpha d\beta^{[l]} \\<br>          \gamma^{[l]} = \gamma^{[l]} - \alpha d\gamma^{[l]}<br>$$        </p>
<h3 id="归一化有效性原理"><a href="#归一化有效性原理" class="headerlink" title="归一化有效性原理"></a>归一化有效性原理</h3><pre><code>1）类似于对输入进行归一化操作后加快梯度下降速度的原理，对隐藏层进行归一化操作有着相似的作用；  
2）Batch 归一化减少了输入值改变的问题,它的确使这些值变得更稳定,神经网络的之后层就会有更坚实的基础。 即使使输入分布改变了一些, 它会改变得更少。 它做的是当前层保持学习,当改变时,迫使后层适应的程度减小了,你可以这样想,它减弱了前层参数的作用与后层参数的作用之间的联系, 它使得网络每层都可以自己学习, 稍稍独立于其它层, 这有助于加速整个网络的学习。  </code></pre><h3 id="Batch-归一化的使用"><a href="#Batch-归一化的使用" class="headerlink" title="Batch 归一化的使用"></a>Batch 归一化的使用</h3><h4 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h4><pre><code>Tensorflow
          <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.batch_normalization()</span><br></pre></td></tr></table></figure></code></pre><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><pre><code>训练时的Batch Norm  </code></pre><p>Batch归一化通常和训练集的mini-batch一起使用。<br>$$<br>\text 按 mini-batch(X^{[i]}) 为单位计算 z^{[l]}、\mu、\sigma^{2}<br>$$</p>
<pre><code>测试时的Batch Norm  </code></pre><p>问题：单个样本的测试如何获得 $\mu 和 \sigma$ ?<br>使用指数加权平均进行估算，这个平均数涵盖所有的mini-batch，计算过程：<br>                1. 计算每个mini-batch的每个隐藏层计算 $\mu^{\{i\}[l]}、\sigma^{2\{i\}[l]}$ ;<br>                2. 使用指数加权平均计算训练过程中得到的 $\mu^{\{i\}[l]}、\sigma^{2\{i\}[l]}$<br>                3. 计算 $z^{ i}_{ norm}、~{z} $</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>线形变换及激活<br>$$<br>Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\<br>t = e^{Z^{[l]}}<br>$$</p>
<p>$$<br>a^{[l]} = \frac{t_{i}}{\sum{_{i=1}^{n}}t_{i}}<br>$$</p>
<p>损失函数<br>$$<br>L(\hat{y}, y) = -\sum_{j=1}^{n}y_{j}\log\hat{y}_{j}<br>$$</p>
<p>代价函数<br>$$<br>J(w^{[l]}, b^{[l]}, … …) = \sum_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)})<br>$$<br>        * softmax 与 hardmax<br>Softmax 这个名称的来源是与所谓 hardmax 对比, hardmax 会把向量 z 变成这个向量[ 1 0 0 0 ], hardmax 函数会观察 z 的元素,然后在 z 中最大元素的位置放上 1,其它位置放上 0。<br>        * 反向传播<br>$$<br>\frac{\partial{L}}{\partial{z}} = dz^{[l]} = \hat{y} - y<br>$$</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/27/third-git-blog/" data-id="clkkvbagk000d7kh7hgrnhmd3" class="article-share-link" data-share="baidu" data-title="第二章 改善深层神经网络：超参数调试、正则化及优化">Share</a>
      

      
        <a href="http://example.com/2023/07/27/third-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-second-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/19/second-git-blog/" class="article-date">
  <time datetime="2023-07-19T08:36:59.000Z" itemprop="datePublished">2023-07-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/second-git-blog/">第一章 神经网络和机器学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="神经网络概要"><a href="#神经网络概要" class="headerlink" title="神经网络概要"></a>神经网络概要</h1><hr>
<p>首先吴恩达通过房价预测的例子，来形象的介绍了神经网络的作用。首先输入X，经过强大的神经网络，可将它映射成Y。<br>吴恩达在课程中也提到神经网络技术概念已经有几十年，为什么最近变得如此热门。笔者在05年研究生学习的时候，当时的课程已经是神经网络和机器学习，那个时候国内研究应用成果的还是比较少，但是理论的研究已经开始兴起，也是受国外很多期刊的影响，那个时候西安交大的徐宗本教授在此领域已经积累了不少经验和成果，后来凭借着这些成果，也当选成中科院院士，当时徐教授任笔者所在学院的院长，所以有幸也听过一些讲座。但随着后来没有从事相关领域的工作，到现在再重新拾起这门课，已经过了十几年的时光了。</p>
<p>回到刚才那个问题，为什么目前神经网络变得如此热门，吴恩达也提到这里面数据规模、计算量及算法的创新都功不可没。</p>
<h1 id="神经网络介绍"><a href="#神经网络介绍" class="headerlink" title="神经网络介绍"></a>神经网络介绍</h1><hr>
<p>为了使得神经网络的概念更好理解，吴恩达通过逻辑回归的例子来类比神经网络，使得我们通过一个简单的例子，了解神经网络的组成。以下图形很好的展示了神经网络的例子，此例子中的神经网络只包含了一个隐藏层。</p>
<p>  <img src="/2023/07/19/second-git-blog/1689757652003.png" alt="神经网络框架图"></p>
<p>  其中输入特征被竖直堆叠起来，叫做神经网络的输入层。另外一层称为隐藏层；最后一层只由一个结点构成，这个被称做输出层，它负责产生预测值。<br>  神经网络的三个要点描述为以下：</p>
<h3 id="输入（Input）-X"><a href="#输入（Input）-X" class="headerlink" title="输入（Input）:X"></a>输入（Input）:X</h3><h3 id="输出-（Output-Y"><a href="#输出-（Output-Y" class="headerlink" title="输出 （Output):Y"></a>输出 （Output):Y</h3><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><h3 id="激活函数-（Activation-Function）"><a href="#激活函数-（Activation-Function）" class="headerlink" title="激活函数 （Activation Function）"></a>激活函数 （Activation Function）</h3><pre><code>使用一个神经网络时，需要决定使用哪种激活函数用在隐藏层上，哪种用在输出节点上。文中介绍比较多的是sigmoid激活函数，实际应用中其他的激活函数效果会更好。激活函数实现了样本特征的非线形变换，如果没有非线形变换，可以证明即使有再多道隐层，神经网络也会“坍缩”成为逻辑回归。

常用激活函数：</code></pre><ul>
<li><p>Tanh<br>$$<br>g(z) = tanh(z)<br>=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \<br>g’(z) = 1 -(tanh(z))^{2}<br>$$</p>
</li>
<li><p>ReLU</p>
</li>
</ul>
<p>$$<br>g(z) = max(0, z) \<br>g’(z) = \begin{cases}<br>0 &amp; \text if  z \lt 0 \<br>1 &amp; \text if  z \ge 0<br>\end{cases}<br>$$</p>
<ul>
<li>Leaky ReLU</li>
</ul>
<p>$$<br>g(z) = max(0.01, z) \<br>g’(z) = \begin{cases}<br>0.01 &amp; \text if  z \lt 0 \<br>1 &amp; \text if z \ge 0<br>\end{cases}<br>$$</p>
<ul>
<li>sigmoid</li>
</ul>
<p>$$<br>g(z) = \frac{1}{1 + e^{-z}} \<br>g’(z) = g(z) \times (1 - g(z))<br>$$</p>
<ul>
<li>softmax</li>
</ul>
<p>$$<br>\text{for } x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}<br>      x_1  &amp;&amp;<br>      x_2 &amp;&amp;<br>      …  &amp;&amp;<br>      x_n<br>    \end{bmatrix}) =<br>     \begin{bmatrix}<br>       \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;<br>      \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;<br>      …  &amp;&amp;<br>      \frac{e^{x_n}}{\sum_{j}e^{x_j}}<br>    \end{bmatrix}  \<br>    \<br>$$</p>
<p>$$<br>softmax(x) = softmax<br>    \begin{bmatrix}<br>      x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \<br>      x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \<br>      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>      x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots  &amp; x_{mn}<br>    \end{bmatrix} =<br>    \begin{bmatrix}<br>      \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots  &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \<br>      \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots  &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \<br>      \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>      \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots  &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}<br>    \end{bmatrix} \<br>  =<br>    \begin{pmatrix}<br>      softmax\text{(first row of x)}  \<br>      softmax\text{(second row of x)} \<br>      …  \<br>      softmax\text{(last row of x)} \<br>    \end{pmatrix}<br>$$</p>
<h3 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数 (Loss Function)"></a>损失函数 (Loss Function)</h3><pre><code>又叫误差函数，用来衡量算法的运行情况，通过这个L称为的损失函数，衡量预测输出值和实际值有多接近。一般用预测值和实际值的平方差或者它们平方差的一半。
  *   L1 损失函数</code></pre><p>$$<br>L_1(\hat{y},y) = \sum_{i=0}^m|(y^{(i)} - \hat{y}^{(i)})|<br>$$</p>
<pre><code>*   L2 损失函数</code></pre><p>$$<br>L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2<br>$$</p>
<pre><code>但通常逻辑回归中，会使用下面的L，</code></pre><p>$$<br>L(a, y) = -(ylog(a) + (1-y)log(1-a))<br>    L’(a,y) = \frac{1-y}{1-a} - \frac{y}{a}<br>$$</p>
<pre><code>这个的好处，以及这门课很多函数也和这个原理类似，就是如果y等于1，尽可能让$\hat&#123;y&#125;$变大，
如果y等于0，尽可能让$\hat&#123;y&#125;$变小</code></pre><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 (Cost Function)"></a>代价函数 (Cost Function)</h3><p>$$<br>\frac{1}{m}\sum_{i=1}^{m}L(a^{(i)}, y^{(i)})<br>$$</p>
<pre><code>损失函数()适用于单个训练样本，代价函数是参数的总代价</code></pre><ul>
<li>参数()：$w$, $b$*</li>
</ul>
<h2 id="逻辑回归与神经网络"><a href="#逻辑回归与神经网络" class="headerlink" title="逻辑回归与神经网络"></a>逻辑回归与神经网络</h2><hr>
<p>关于逻辑回归和神经网络类比，有以下的结论：其中逻辑回归可以看作是单层（没有隐藏层）的二分类神经网络，其中<br>  输入特征向量X，预测目标为0或1<br>  所以逻辑回归是给出输入x以及参数w和b之后，如何输出预测值$\hat{y}$。</p>
<p>  神经网络可以从两个方面看作是逻辑回归的叠加：</p>
<ul>
<li><p>层数的叠加<br>神经网络有多个隐层，每层有自己独立的参数。</p>
</li>
<li><p>特征参数的叠加<br>神经网络每一层在传入各层激活函数前，会进行多种不同的线形变换，每种变换经激活函数计算后形成该层一个神经元，一个神经元对应一次逻辑回归计算，同时也对应了参数矩阵一行。</p>
<p>以下是关于逻辑回归的训练目标与训练过程的介绍</p>
</li>
</ul>
<h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><hr>
<p>  逻辑回归作为二分类算法，其损失函数的数学意义可以理解为预测值$\hat{y}$为真实值$y$的概率，即：</p>
<p>$$<br>y = 1: p(y|x) =\hat{y}y = 0: p(y|x) = 1 - \hat{y}<br>$$</p>
<pre><code>将两个公式合并为一个公式：</code></pre><p>$$<br>p(y|x) = \hat{y}^{y}(1-\hat{y})^{(1-y)}<br>$$</p>
<pre><code>基于上述公式，我们需要 p(y|x) 所得概率值越大越好(从而达到预测值与真实值逼近的目的)。为避免进行指数运算所带来的复杂性，利用对数函数的严格单调性，对上述公式两边同时取对数，转换为下面的公式：</code></pre><p>$$<br>\log{p(y|x)} = y\log{\hat{y}} + (1-y)\log(1-\hat{y})<br>    = -L(\hat{y}, y)<br>$$</p>
<pre><code>概率值越大，损失值应该越小，二者应是相反关系且概率值都是小于1，其对数为负，所以损失值取负。
训练的目标就是通过寻找参数$w$, $b$,以获得最小的损失值。每次迭代时参数更新公式为：</code></pre><p>$$<br>w = w - \alpha\times dw<br>    b = b - \alpha\times db<br>$$</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><hr>
<p>   寻找最优参数过程基于凸函数的梯度下降()原理，通过对训练集迭代进行“正向传播()”和“反向传播()”完成。</p>
<h4 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h4><p>$$<br>    \text 根据样本数据 x 推导出 \hat{y},并保留中间值z用于反向传播的计算；<br>$$</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>$$<br>\frac{da}{dz} = a \times(1 - a)\\<br>    da = \frac{dL}{da} = \frac{1-y}{1-a} - \frac{y}{a} \\<br>    dz =\frac{dL}{dz} = \frac{dL}{da} \times \frac{da}{dz} = a - y\\<br>    \frac{dz}{dw} = x\\<br>    \frac{dz}{db} = 1 \\<br>    dw = \frac{dL}{dw} = \frac{dL}{dz} \times \frac{dz}{dw} = (a-y) \times x \\<br>    db = \frac{dL}{db} = \frac{dL}{dz} \times \frac{dz}{db} = a - y<br>$$</p>
<ul>
<li>多样本向量化计算<br>使用向量化计算比循环运算快约400倍。<br>$$<br>$$<br>输入为一个$（n，m）$的矩阵，m列的列向量，每个样本为$n$个特征值；<br>参数$w$维度为：（1，n),参数$b$维度为：（1，1)</li>
</ul>
<h2 id="神经网络训练"><a href="#神经网络训练" class="headerlink" title="神经网络训练"></a>神经网络训练</h2><h3 id="向量化运算"><a href="#向量化运算" class="headerlink" title="向量化运算"></a>向量化运算</h3><p>  向量在网络各层计算过程中的维度规律：<br>$$<br>$$<br>    * 各层样本个数保持不变，即：$A^{[i]}$、$Z^{[i]}$ 的列值均保持为m(样本数)，变化的是各层每个样本的特征数(行数)<br>    * 参数 $W^{[i]}$的维度与样本数无关，行为本层需生成的样本特征数，列为上层特征数；$B^{[i]}$为本层特征数，一列<br>    * 对于反向传播时的导数向量同样如此。</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><pre><code>#### 逻辑回归 参数初始化
  不存在中间隐层，输出层也只有一个神经元，参数可以初始化为任意值，考虑到激活函数曲线的斜率特性，通常都初始化为0。
#### 神经网络 参数初始化
  神经网络因隐层存在多个神经元，如参数初始化为0，则可证明每个神经元作用是等价的，因此，神经网络参数需初始化为随机值，且乘以一个较小的数，如：0.01，以使计算从斜率较大的地方开始。

* 另一项AI通用技术-数据归一化：训练数据归一化后将提高梯度下降的性能。举例, 如果：</code></pre><p>$$<br>x = \begin{bmatrix}<br>      0 &amp; 3 &amp; 4\<br>      2 &amp; 6 &amp; 4 \<br>      \end{bmatrix}<br>$$</p>
<p>then |x| = np.linalg.norm(x, axis = 1, keepdims = True)</p>
<p>$$<br>    = \begin{bmatrix}<br>      5\<br>      \sqrt{56} \<br>      \end{bmatrix}<br>$$</p>
<p>$$<br>    x_{normalized} = \frac{x}{|x|} =<br>      \begin{bmatrix}<br>      0 &amp; \frac{3}{5} &amp; \frac{4}{5} \<br>      \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \<br>      \end{bmatrix}<br>$$</p>
<h3 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h3><p>  上述介绍了神经网络的训练目标和训练过程，接下来我们来介绍，为什么要使用深层神经网络？<br>    过去实践经验，有些函数，只有非常深的神经网络能学会，而浅的模型办不到。<br>    同样课堂上将深度神经网络和人类大脑相比较，即先探测简单的东西（开始的层获取底层特征），组合起来才能探测复杂的物体（底层特征组合后的特征），从功能上讲，浅层网络也能达到同样效果，但是计算的效率较低。<br>    有两个特点<br>    small: 隐藏单元的数量相对较少<br>    deep: 隐藏层数目比较多<br>    深层网络隐藏单元数量相对较少，隐藏层数较多。如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<ul>
<li><p>参数      超参数：<br>$$<br>$$</p>
<p>算法中的学习率：learning rate $\alpha$<br>梯度下降循环的数量:n<br>隐藏层数目：L<br>隐藏层单元数目(): $n^[l]$<br>激活函数的选择<br>需要操作者自行设置的，这些数字实际上控制了最后的参数$w$, $b$，称之为超参数<br>按下来第二门课将会详细介绍参数的选择、正则化以及优化。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/19/second-git-blog/" data-id="clkkvbagi00067kh7flc3et0q" class="article-share-link" data-share="baidu" data-title="第一章 神经网络和机器学习">Share</a>
      

      
        <a href="http://example.com/2023/07/19/second-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/18/hello-world/" class="article-date">
  <time datetime="2023-07-18T03:54:17.414Z" itemprop="datePublished">2023-07-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/18/hello-world/" data-id="clkkvbaga00007kh75zud5utx" class="article-share-link" data-share="baidu" data-title="Hello World">Share</a>
      

      
        <a href="http://example.com/2023/07/18/hello-world/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  
    <article id="post-first-git-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/17/first-git-blog/" class="article-date">
  <time datetime="2023-07-17T08:36:59.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/first-git-blog/">前言</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="写在博客前面"><a href="#写在博客前面" class="headerlink" title="写在博客前面"></a><strong>写在博客前面</strong></h1><hr>
<p>前后差不多将近一年半的时间（从2021年的10月份从网上看到深度学习的经典之作开始学习），比较曲折，耗时较长，中间还经历过上海封控等因素（2022.3-6），同时大家因为也有正常的工作要做，留给深度学习的时间比较零碎，但其实去年的时候（2022.5），已经完成了主体部分的学习。后来计划是写些东西，首先要整理笔记，把之前各门科的笔记和对应作业的要点整理起来，差不多在今年过年时候完成（2023.2）。然后这个写作的任务就落在我的头上，加上今年公司的改革，我的工作上半年特别忙碌，所以笔记拖到现(2023.5)在。最近是两个项目的空档时间，抓紧利用这个时间可以快速把之前的学习经历、心得和作业要点通通梳理，也算是为这次学习画上完满的句号。之后将是针对所学内容的开发工具或者相关算法的改进，这个是后话，也希望学到的东西可以持续的应用起来。</p>
<h1 id="为什么选择这套教材"><a href="#为什么选择这套教材" class="headerlink" title="为什么选择这套教材"></a><strong>为什么选择这套教材</strong></h1><hr>
<p>深度挖掘是当今科技界比较热门的技能，对于想入门的同学，除了要掌握一般的编程知识，熟悉机器学习的基本概念，还需要有python编程的基础。但即使掌握了这些技能，如何选择一门好的入门课程仍然困扰着大家。例如笔者一开始的时候，网上购买了一些付费课程，但是网上的课程鱼龙混杂，跟着视频学习的时候，发现遇到了更多的问题，另外课程上存在着各种只是简单的复制，没有注重课程的连续性，或者是其中一些必要的知识，没有介绍清楚，导致学习起来特别痛苦。花了很多时间去调查中间的过程。<br>在中间探索的过程中，发现了吴恩达YYDS的深度挖掘的课程，但是由于笔者和朋友已经不接触英语好久了，对于全英文的课程，心里还是存着敬畏。感觉没有勇气能够坚持完全部课程的学习。后来发现了已经翻译成中文的学习笔记和网易云课程已经将视频做了翻译，这样就扫清了前期学习的语言障碍。<br>吴恩达这些课程，总共有5堂课，通过课程的学习，可以学习到深度学习的基础，学会构建神经网络。并且可以在吴恩达本人及多位业务顶尖专家的指导下创建自己的机器 学习项目。其中对卷积神经网络(CNN)、递归神经网络（RNN）、长短期记忆（LSTM）等深度学习都有所涉及。每节课都配有习题和编程练习，之后我们将会分章节，解析习题和编程练习，以便大家可以更好地了解深度挖掘的原理和应用。</p>
<h1 id="章节内容介绍"><a href="#章节内容介绍" class="headerlink" title="章节内容介绍"></a><strong>章节内容介绍</strong></h1><hr>
<p>主要是包括五门课的内容，主要的内容和结构如下:<br>文章的脉落也比较清晰，第一门课是对于神经网络用到的基础进行阐述，同时介绍了浅层网络和深层网络的基础，在大家对于这些概念有些基本了解后，开始进行深入探索。在第二门课时，对于深层神经网络的参数调节、正则化以及优化做了详细的阐述，紧接着是第三门课对于结构化机器学习项目的学习策略进行介绍，主要是基于作者在日常的项目中遇到的问题以及大家经常关心的策略等有了比较详细的介绍。之后在第四门课时介绍了卷积神经网络，以及目标检测的应用，通过两个特殊应用：人脸识别和神经风格的转换，介绍了卷积神经网络的应用。最后一门课第五门课是关于序列模型的介绍，这块对于目前自然语言的应用等领域有着非常好的借鉴作用。</p>
<h2 id="章节列表"><a href="#章节列表" class="headerlink" title="章节列表"></a><strong>章节列表</strong></h2><hr>
<h3 id="第一门课-神经网络和深度学习"><a href="#第一门课-神经网络和深度学习" class="headerlink" title="第一门课  神经网络和深度学习"></a>第一门课  神经网络和深度学习</h3><pre><code>- 深度学习引言
- 神经网络的编程基础
- 浅层神经网络
- 深层神经网络</code></pre><h3 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化" class="headerlink" title="第二门课  改善深层神经网络：超参数调试、正则化以及优化"></a>第二门课  改善深层神经网络：超参数调试、正则化以及优化</h3><pre><code>- 深度学习的实践层面
- 优化算法
- 超参数调试、Batch正则化和程序框架</code></pre><h3 id="第三门课-结构化机器学习项目"><a href="#第三门课-结构化机器学习项目" class="headerlink" title="第三门课 结构化机器学习项目"></a>第三门课 结构化机器学习项目</h3><pre><code>- 机器学习策略1
- 机器学习策略2</code></pre><h3 id="第四门课-卷积神经网络"><a href="#第四门课-卷积神经网络" class="headerlink" title="第四门课  卷积神经网络"></a>第四门课  卷积神经网络</h3><pre><code>- 卷积神经网络
- 深度卷积网络
- 目标检测
- 特殊应用：人脸识别和神经风格转换</code></pre><h3 id="第五门课-序列模型"><a href="#第五门课-序列模型" class="headerlink" title="第五门课  序列模型"></a>第五门课  序列模型</h3><pre><code>- 循环序列模型
- 自然语言处理与词嵌入
- 序列模型和注意力机制</code></pre>
      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/17/first-git-blog/" data-id="clkkvbage00017kh72kc8ax8m" class="article-share-link" data-share="baidu" data-title="前言">Share</a>
      

      
        <a href="http://example.com/2023/07/17/first-git-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  
    <article id="post-my-first-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/07/17/my-first-blog/" class="article-date">
  <time datetime="2023-07-17T07:00:58.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/my-first-blog/">Hexo基本使用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hexo的优点：快速、简洁且高效的博客框架。Hexo使用Markdown解析文章，在几秒内，即可利用按照指定主题生成静态网页。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装使用hexo之前需要先安装Node.js和Git，当已经安装了Node.js和npm(npm是node.js的包管理工具)，可以通过以下命令安装hexo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p>可以通过以下命令查看主机中是否安装了node.js和npm</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ node --version    <span class="comment">#检查是否安装了node.js</span></span><br><span class="line">$ npm --version     <span class="comment">#检查是否安装了npm</span></span><br></pre></td></tr></table></figure>

<p>如果已经安装，提示如下：</p>
<p>$ C:\Users\YingZhou\blog&gt;node –version<br>v18.16.1</p>
<p>C:\Users\YingZhou\blog&gt;npm –version<br>9.5.1</p>
<h1 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h1><p>安装完Hexo之后，执行下列命令，Hexo将会在指定目录中新建所需要的文件，指定的目录即为Hexo的工作站</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>


<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://example.com/2023/07/17/my-first-blog/" data-id="clkkvbagg00037kh70xpx8mew" class="article-share-link" data-share="baidu" data-title="Hexo基本使用">Share</a>
      

      
        <a href="http://example.com/2023/07/17/my-first-blog/#ds-thread" class="article-comment-link">Comments</a>
      

      
    </footer>
  </div>
  
</article>


  

</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82/">参数调节</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" rel="tag">深度挖掘</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%95%AA%E5%A4%96%E7%AF%87/" rel="tag">番外篇</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%8C%96%E6%8E%98/" style="font-size: 20px;">深度挖掘</a> <a href="/tags/%E7%95%AA%E5%A4%96%E7%AF%87/" style="font-size: 10px;">番外篇</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">6</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/27/hexo-equation-blog/">Hexo如何解决公式渲染问题</a>
          </li>
        
          <li>
            <a href="/2023/07/27/third-git-blog/">第二章 改善深层神经网络：超参数调试、正则化及优化</a>
          </li>
        
          <li>
            <a href="/2023/07/19/second-git-blog/">第一章 神经网络和机器学习</a>
          </li>
        
          <li>
            <a href="/2023/07/18/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2023/07/17/first-git-blog/">前言</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://arvinxiang.com" target="_blank">主题作者</a>
          </li>
        
          <li>
            <a href="http://reqianduan.com" target="_blank">热前端</a>
          </li>
        
          <li>
            <a href="http://yuancheng.work" target="_blank">远程.work</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->

<!-- 多说公共js代码 start -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"reqianduan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共js代码 end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
